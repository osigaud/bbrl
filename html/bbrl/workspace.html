<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>bbrl.workspace API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>bbrl.workspace</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#


from __future__ import annotations

import numpy as np
import torch
from typing import Optional

&#34;&#34;&#34; This module provides different ways to store tensors that are more flexible than the torch.Tensor class.
It also defines the `Workspace` as a dictionary of tensors and a version of the workspace
where tensors are in shared memory for multiprocessing
&#34;&#34;&#34;


class SlicedTemporalTensor:
    &#34;&#34;&#34;A SlicedTemporalTensor represents a tensor of size TxBx... by using a list of tensors of size Bx...
    The interest is that this tensor automatically adapts its timestep dimension and does not need to have a predefined size.
    &#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;Initialize an empty tensor&#34;&#34;&#34;
        self.tensors: list[torch.Tensor] = []
        self.size: torch.Size = None
        self.device: torch.device = None
        self.dtype: torch.dtype = None

    def set(self, t: int, value: torch.Tensor, batch_dims: Optional[tuple[int, int]]):
        &#34;&#34;&#34;Set a value (dim Bx...) at time t&#34;&#34;&#34;
        assert (
            batch_dims is None
        ), &#34;Unable to use batch dimensions with SlicedTemporalTensor&#34;
        if self.size is None:
            self.size = value.size()
            self.device = value.device
            self.dtype = value.dtype
        assert self.size == value.size(), (
            &#34;Incompatible size:&#34; + str(self.size) + &#34; vs &#34; + str(value.size())
        )
        assert self.device == value.device, &#34;Incompatible device&#34;
        assert self.dtype == value.dtype, &#34;Incompatible type&#34;
        while len(self.tensors) &lt;= t:
            self.tensors.append(
                torch.zeros(*self.size, device=self.device, dtype=self.dtype)
            )
        self.tensors[t] = value

    def to(self, device: torch.device):
        &#34;&#34;&#34;Move the tensor to a specific device&#34;&#34;&#34;
        s = SlicedTemporalTensor()
        for k in range(len(self.tensors)):
            s.set(k, self.tensors[k].to(device))
        return s

    def get(self, t: int, batch_dims: Optional[tuple[int, int]]):
        &#34;&#34;&#34;Get the value of the tensor at time t&#34;&#34;&#34;

        assert (
            batch_dims is None
        ), &#34;Unable to use batch dimensions with SlicedTemporalTensor&#34;
        assert t &lt; len(self.tensors), &#34;Temporal index out of bounds&#34;
        return self.tensors[t]

    def get_full(self, batch_dims):
        &#34;&#34;&#34;Returns the complete tensor of size TxBx...&#34;&#34;&#34;

        assert (
            batch_dims is None
        ), &#34;Unable to use batch dimensions with SlicedTemporalTensor&#34;
        return torch.cat([a.unsqueeze(0) for a in self.tensors], dim=0)

    def get_time_truncated(
        self, from_time: int, to_time: int, batch_dims: Optional[tuple[int, int]]
    ):
        &#34;&#34;&#34;Returns tensor[from_time:to_time]&#34;&#34;&#34;
        assert from_time &gt;= 0 and to_time &gt;= 0 and to_time &gt; from_time
        assert batch_dims is None
        return torch.cat(
            [
                self.tensors[k].unsqueeze(0)
                for k in range(from_time, min(len(self.tensors), to_time))
            ],
            dim=0,
        )

    def set_full(self, value: torch.Tensor, batch_dims: Optional[tuple[int, int]]):
        &#34;&#34;&#34;Set the tensor given a BxTx... tensor.
        The input tensor is cut into slices that are stored in a list of tensors
        &#34;&#34;&#34;
        assert (
            batch_dims is None
        ), &#34;Unable to use batch dimensions with SlicedTemporalTensor&#34;
        for t in range(value.size()[0]):
            self.set(t, value[t], batch_dims=batch_dims)

    def time_size(self):
        &#34;&#34;&#34;
        Return the size of the time dimension
        &#34;&#34;&#34;
        return len(self.tensors)

    def batch_size(self):
        &#34;&#34;&#34;Return the size of the batch dimension&#34;&#34;&#34;
        return self.tensors[0].size()[0]

    def select_batch(self, batch_indexes: torch.LongTensor):
        &#34;&#34;&#34;Return the tensor where the batch dimension has been selected by the index&#34;&#34;&#34;
        var = SlicedTemporalTensor()
        for t, v in enumerate(self.tensors):
            batch_indexes = batch_indexes.to(v.device)
            var.set(t, v[batch_indexes], None)
        return var

    def clear(self):
        &#34;&#34;&#34;Clear the tensor&#34;&#34;&#34;
        self.tensors = []
        self.size = None
        self.device = None
        self.dtype = None

    def copy_time(self, from_time: int, to_time: int, n_steps: int):
        &#34;&#34;&#34;Copy temporal slices of the tensor from from_time:from_time+n_steps to to_time:to_time+n_steps&#34;&#34;&#34;
        for t in range(n_steps):
            v = self.get(from_time + t, batch_dims=None)
            self.set(to_time + t, v, batch_dims=None)

    def subtime(self, from_t: int, to_t: int):
        &#34;&#34;&#34;
        Return tensor[from_t:to_t]

        &#34;&#34;&#34;
        return CompactTemporalTensor(
            torch.cat([a.unsqueeze(0) for a in self.tensors[from_t:to_t]], dim=0)
        )

    def zero_grad(self):
        &#34;&#34;&#34;Clear any gradient information in the tensor&#34;&#34;&#34;
        self.tensors = [v.detach() for v in self.tensors]


class CompactTemporalTensor:
    &#34;&#34;&#34;A CompactTemporalTensor is a tensor of size TxBx...
    It behaves like the `SlicedTemporalTensor` but has a fixed size that cannot change.
    It is faster than the SlicedTemporalTensor.
        See `SlicedTemporalTensor`
    &#34;&#34;&#34;

    def __init__(self, value: torch.Tensor = None):
        self.size = None
        self.device = None
        self.dtype = None
        self.tensor = None
        if value is not None:
            self.tensor = value
            self.device = value.device
            self.size = value.size()
            self.dtype = value.dtype

    def set(self, t, value, batch_dims):
        assert False
        assert self.tensor is not None, &#34;Tensor must be initialized&#34;
        assert self.size[1:] == value.size(), &#34;Incompatible size&#34;
        assert self.device == value.device, &#34;Incompatible device&#34;
        assert self.dtype == value.dtype, &#34;Incompatible type&#34;
        assert t &lt; self.tensor.size()[0], &#34;Temporal index out of bounds&#34;
        if batch_dims is None:
            self.tensor[t] = value
        else:
            self.tensor[t, batch_dims[0] : batch_dims[1]] = value

    def select_batch(self, batch_indexes):
        v = CompactTemporalTensor(self.tensor[:, batch_indexes])
        return v

    def to_sliced(self) -&gt; SlicedTemporalTensor:
        &#34;&#34;&#34;Transform the tensor to a `SlicedTemporalTensor`&#34;&#34;&#34;
        v = SlicedTemporalTensor()
        for t in range(self.tensor.size()[0]):
            v.set(t, self.tensor[t], None)
        return v

    def to(self, device):
        if device == self.tensor.device:
            return self
        t = self.tensor.to(device)
        return CompactTemporalTensor(t)

    def get(self, t, batch_dims):
        assert t &lt; self.tensor.size()[0], &#34;Temporal index out of bounds&#34;
        if batch_dims is None:
            return self.tensor[t]
        else:
            return self.tensor[t, batch_dims[0] : batch_dims[1]]

    def get_full(self, batch_dims):
        if batch_dims is None:
            return self.tensor
        else:
            return self.tensor[:, batch_dims[0] : batch_dims[1]]

    def time_size(self):
        return self.tensor.size()[0]

    def batch_size(self):
        return self.tensor.size()[1]

    def set_full(self, value, batch_dims):
        if self.tensor is None:
            assert batch_dims is None
            self.size = value.size()
            self.dtype = value.dtype
            self.device = value.device
        if batch_dims is None:
            self.tensor = value
        else:
            self.tensor[:, batch_dims[0] : batch_dims[1]] = value

    def subtime(self, from_t, to_t):
        return CompactTemporalTensor(self.tensor[from_t:to_t])

    def clear(self):
        self.size = None
        self.device = None
        self.dtype = None
        self.tensor = None

    def copy_time(self, from_time, to_time, n_steps):
        self.tensor[to_time : to_time + n_steps] = self.tensor[
            from_time : from_time + n_steps
        ]

    def zero_grad(self):
        self.tensor = self.tensor.detach()


class CompactSharedTensor:
    &#34;&#34;&#34;It corresponds to a tensor in shared memory.
    It is used when building a workspace shared by multiple processes.
        All the methods behaves like the methods of `SlicedTemporalTensor`
    &#34;&#34;&#34;

    def __init__(self, _tensor: torch.Tensor):
        self.tensor = _tensor
        self.tensor.share_memory_()

    def set(self, t, value, batch_dims):
        if batch_dims is None:
            self.tensor[t] = value.detach()
        else:
            self.tensor[t, batch_dims[0] : batch_dims[1]] = value.detach()

    def get(self, t, batch_dims):
        assert t &lt; self.tensor.size()[0], &#34;Temporal index out of bounds&#34;
        if batch_dims is None:
            return self.tensor[t]
        else:
            return self.tensor[t, batch_dims[0] : batch_dims[1]]

    def to(self, device):
        if device == self.tensor.device:
            return self
        t = self.tensor.to(device)
        t.share_memory_()
        return CompactSharedTensor(t)

    def select_batch(self, batch_indexes):
        v = CompactSharedTensor(self.tensor[:, batch_indexes])
        return v

    def get_full(self, batch_dims):
        if batch_dims is None:
            return self.tensor
        else:
            return self.tensor[:, batch_dims[0] : batch_dims[1]]

    def time_size(self):
        return self.tensor.size()[0]

    def batch_size(self):
        return self.tensor.size()[1]

    def set_full(self, value, batch_dims):
        if batch_dims is None:
            self.tensor = value.detach()
        else:
            self.tensor[:, batch_dims[0] : batch_dims[1]] = value.detach()

    def clear(self):
        assert False, &#34;Cannot clear a shared tensor&#34;

    def subtime(self, from_t, to_t):
        t = self.tensor[from_t:to_t]
        return CompactSharedTensor(t)

    def copy_time(self, from_time, to_time, n_steps):
        self.tensor[to_time : to_time + n_steps] = self.tensor[
            from_time : from_time + n_steps
        ]

    def zero_grad(self):
        pass


def take_per_row_strided(a, index, num_elem=2):
    # TODO: Optimize this function
    all_index = index
    arange = torch.arange(a.size()[1], device=a.device)
    return torch.cat(
        [a[all_index + t, arange].unsqueeze(0) for t in range(num_elem)], dim=0
    )


class Workspace:
    &#34;&#34;&#34;Workspace is the most important class in `bbrl`.
    It correponds to a collection of tensors
    (&#39;SlicedTemporalTensor`, `CompactTemporalTensor` or ` CompactSharedTensor`).
    In the majority of cases, we consider that all the tensors have the same time and batch sizes
    (but it is not mandatory for most of the functions)
    &#34;&#34;&#34;

    def __init__(self, workspace: Optional[Workspace] = None):
        &#34;&#34;&#34;Create an empty workspace

        Args:
            workspace (Workspace, optional): If specified, it creates a copy of the workspace
        (where tensors are cloned as CompactTemporalTensors)
        &#34;&#34;&#34;
        self.variables = {}
        self.is_shared = False
        if workspace is not None:
            for k in workspace.keys():
                self.set_full(k, workspace[k].clone())

    def set(
        self,
        var_name: str,
        t: int,
        v: torch.Tensor,
        batch_dims: Optional[tuple[int, int]] = None,
    ):
        &#34;&#34;&#34;Set the variable var_name at time t&#34;&#34;&#34;
        if var_name not in self.variables:
            assert not self.is_shared, &#34;Cannot add new variable into a shared workspace&#34;
            self.variables[var_name] = SlicedTemporalTensor()
        elif isinstance(self.variables[var_name], CompactTemporalTensor):
            self.variables[var_name] = self.variables[var_name].to_sliced()

        self.variables[var_name].set(t, v, batch_dims=batch_dims)

    def get(
        self, var_name: str, t: int, batch_dims: Optional[tuple[int, int]] = None
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;Get the variable var_name at time t&#34;&#34;&#34;
        assert var_name in self.variables, &#34;Unknown variable &#39;&#34; + var_name + &#34;&#39;&#34;
        return self.variables[var_name].get(t, batch_dims=batch_dims)

    def clear(self, name=None):
        &#34;&#34;&#34;Remove all the variables from the workspace&#34;&#34;&#34;
        if name is None:
            for k, v in self.variables.items():
                v.clear()
            self.variables = {}
        else:
            self.variables[name].clear()
            del self.variables[name]

    def contiguous(self) -&gt; Workspace:
        &#34;&#34;&#34;Generates a workspace where all tensors are stored in the Compact format.&#34;&#34;&#34;
        workspace = Workspace()
        for k in self.keys():
            workspace.set_full(k, self.get_full(k))
        return workspace

    def set_full(
        self,
        var_name: str,
        value: torch.Tensor,
        batch_dims: Optional[tuple[int, int]] = None,
    ):
        &#34;&#34;&#34;Set variable var_name with a complete tensor (TxBx...) where T is the time dimension
        and B is the batch size
        &#34;&#34;&#34;
        if var_name not in self.variables:
            assert not self.is_shared, &#34;Cannot add new variable into a shared workspace&#34;
            self.variables[var_name] = CompactTemporalTensor()
        self.variables[var_name].set_full(value, batch_dims=batch_dims)

    def get_full(
        self, var_name: str, batch_dims: Optional[tuple[int, int]] = None
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;Return the complete tensor for var_name&#34;&#34;&#34;
        assert var_name in self.variables, (
            &#34;[Workspace.get_full] unknown variable &#39;&#34; + var_name + &#34;&#39;&#34;
        )
        return self.variables[var_name].get_full(batch_dims=batch_dims)

    def keys(self):
        &#34;&#34;&#34;Return an iterator over the variables names&#34;&#34;&#34;
        return self.variables.keys()

    def __getitem__(self, key):
        &#34;&#34;&#34;If key is a string, then it returns a torch.Tensor
        If key is a list of string, it returns a tuple of torch.Tensor
        &#34;&#34;&#34;
        if isinstance(key, str):
            return self.get_full(key, None)
        else:
            return (self.get_full(k, None) for k in key)

    def _all_variables_same_time_size(self) -&gt; bool:
        &#34;&#34;&#34;Check that all variables have the same time size&#34;&#34;&#34;
        _ts = None
        for k, v in self.variables.items():
            if _ts is None:
                _ts = v.time_size()
            if _ts != v.time_size():
                return False
        return True

    def time_size(self) -&gt; int:
        &#34;&#34;&#34;Return the time size of the variables in the workspace&#34;&#34;&#34;
        _ts = None
        for k, v in self.variables.items():
            if _ts is None:
                _ts = v.time_size()
            assert _ts == v.time_size(), &#34;Variables must have the same time size&#34;
        return _ts

    def batch_size(self) -&gt; int:
        &#34;&#34;&#34;Return the batch size of the variables in the workspace&#34;&#34;&#34;
        _bs = None
        for k, v in self.variables.items():
            if _bs is None:
                _bs = v.batch_size()
            assert _bs == v.batch_size(), &#34;Variables must have the same batch size&#34;
        return _bs

    def select_batch(self, batch_indexes: torch.LongTensor) -&gt; Workspace:
        &#34;&#34;&#34;Given a tensor of indexes, returns a new workspace
        with the selected elements (over the batch dimension)
        &#34;&#34;&#34;
        _bs = None
        for k, v in self.variables.items():
            if _bs is None:
                _bs = v.batch_size()
            assert _bs == v.batch_size(), &#34;Variables must have the same batch size&#34;

        workspace = Workspace()
        for k, v in self.variables.items():
            v = v.select_batch(batch_indexes)
            workspace.variables[k] = v
        return workspace

    def select_batch_n(self, n):
        &#34;&#34;&#34;Return a new Workspace of batch_size==n by randomly sampling over the batch dimensions&#34;&#34;&#34;
        who = torch.randint(low=0, high=self.batch_size(), size=(n,))
        return self.select_batch(who)

    def copy_time(
        self,
        from_time: int,
        to_time: int,
        n_steps: int,
        var_names: Optional[list[str]] = None,
    ):
        &#34;&#34;&#34;Copy all the variables values from time `from_time` to `from_time+n_steps`
        to `to_time` to `to_time+n_steps`
        It can be restricted to specific variables using `var_names`.
        &#34;&#34;&#34;
        for k, v in self.variables.items():
            if var_names is None or k in var_names:
                v.copy_time(from_time, to_time, n_steps)

    def get_time_truncated(
        self,
        var_name: str,
        from_time: int,
        to_time: int,
        batch_dims: Optional[tuple[int, int]] = None,
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;Return workspace[var_name][from_time:to_time]&#34;&#34;&#34;
        assert from_time &gt;= 0 and to_time &gt;= 0 and to_time &gt; from_time

        v = self.variables[var_name]
        if isinstance(v, SlicedTemporalTensor):
            return v.get_time_truncated(from_time, to_time, batch_dims)
        else:
            return v.get_full(batch_dims)[from_time:to_time]

    def get_time_truncated_workspace(self, from_time: int, to_time: int) -&gt; Workspace:
        &#34;&#34;&#34;Return a workspace where all variables are truncated between from_time and to_time&#34;&#34;&#34;
        workspace = Workspace()
        for k in self.keys():
            workspace.set_full(k, self.get_time_truncated(k, from_time, to_time, None))
        return workspace

    # Static function
    def cat_batch(self, workspaces: list[Workspace]) -&gt; Workspace:
        &#34;&#34;&#34;Concatenate multiple workspaces over the batch dimension.
        The workspaces must have the same time dimension.
        &#34;&#34;&#34;

        ts = None
        for w in workspaces:
            if ts is None:
                ts = w.time_size()
            assert ts == w.time_size(), &#34;Workspaces must have the same time size&#34;

        workspace = Workspace()
        for k in workspaces[0].keys():
            vals = [w[k] for w in workspaces]
            v = torch.cat(vals, dim=1)
            workspace.set_full(k, v)
        return workspace

    def copy_n_last_steps(self, n: int, var_names: Optional[list[str]] = None) -&gt; None:
        &#34;&#34;&#34;Copy the n last timesteps of each variable to the n first timesteps.&#34;&#34;&#34;
        _ts = None
        for k, v in self.variables.items():
            if var_names is None or k in var_names:
                if _ts is None:
                    _ts = v.time_size()
                assert _ts == v.time_size(), (
                    &#34;Variables must have the same time size: &#34;
                    + str(_ts)
                    + &#34; vs &#34;
                    + str(v.time_size())
                )

        for k, v in self.variables.items():
            if var_names is None or k in var_names:
                self.copy_time(_ts - n, 0, n)

    def zero_grad(self) -&gt; None:
        &#34;&#34;&#34;Remove any gradient information&#34;&#34;&#34;
        for k, v in self.variables.items():
            v.zero_grad()

    def to(self, device: torch.device) -&gt; Workspace:
        &#34;&#34;&#34;Return a workspace where all tensors are on a particular device&#34;&#34;&#34;
        workspace = Workspace()
        for k, v in self.variables.items():
            workspace.variables[k] = v.to(device)
        return workspace

    def _convert_to_shared_workspace(self, n_repeat=1, time_size=None):
        &#34;&#34;&#34;INTERNAL METHOD.
        It converts a workspace to a shared workspace, by repeating this workspace n times on the batch dimension
        It also automatically adapts the time_size if specified (used in NRemoteAgent.create)
        &#34;&#34;&#34;

        with torch.no_grad():
            workspace = Workspace()
            for k, v in self.variables.items():
                value = v.get_full(None).detach()
                if time_size is not None:
                    s = value.size()
                    value = torch.zeros(
                        time_size, *s[1:], dtype=value.dtype, device=value.device
                    )
                ts = [value for _ in range(n_repeat)]
                value = torch.cat(ts, dim=1)
                workspace.variables[k] = CompactSharedTensor(value)
                workspace.is_shared = True
        return workspace

    def subtime(self, from_t: int, to_t: int) -&gt; Workspace:
        &#34;&#34;&#34;
        Return a workspace restricted to a subset of the time dimension
        &#34;&#34;&#34;
        assert (
            self._all_variables_same_time_size()
        ), &#34;All variables must have the same time size&#34;
        workspace = Workspace()
        for k, v in self.variables.items():
            workspace.variables[k] = v.subtime(from_t, to_t)
        return workspace

    def remove_variable(self, var_name: str):
        &#34;&#34;&#34;Remove a variable from the Workspace&#34;&#34;&#34;
        del self.variables[var_name]

    def __str__(self):
        r = [&#34;Workspace:&#34;]
        for k, v in self.variables.items():
            r.append(
                &#34;\t&#34;
                + k
                + &#34;: time_size = &#34;
                + str(v.time_size())
                + &#34;, batch_size = &#34;
                + str(v.batch_size())
            )
        return &#34;\n&#34;.join(r)

    def select_subtime(self, t: torch.LongTensor, window_size: int) -&gt; Workspace:
        &#34;&#34;&#34;
        `t` is a tensor of size `batch_size` that provides one time index for each element of the workspace.
        Then the function returns a new workspace by aggregating `window_size` timesteps starting from index `t`
        This methods allows to sample multiple windows in the Workspace.
        Note that the function may be quite slow.
        &#34;&#34;&#34;
        _vars = {k: v.get_full(batch_dims=None) for k, v in self.variables.items()}
        workspace = Workspace()
        for k, v in _vars.items():
            workspace.set_full(
                k, take_per_row_strided(v, t, num_elem=window_size), batch_dims=None
            )
        return workspace

    # Static
    def sample_subworkspace(self, n_times, n_batch_elements, n_timesteps):
        &#34;&#34;&#34;Sample a workspace from the  workspace. The process is the following:
                * Let us consider that workspace batch_size is B and time_size is T
                * For n_times iterations:
                    * We sample a time window of size n_timesteps
                    * We then sample a n_batch_elements elements on the batch size
                    * =&gt;&gt; we obtain a worspace of size n_batch_elements x n_timesteps
                * We concatenate all the workspaces collected (over the batch dimension)

        Args:
            n_times ([type]): The number of sub workspaces to sample (and concatenate)
            n_batch_elements ([type]): &lt;=workspace.batch_size(): nb of batch elements to sample for each subworkspace
            n_timesteps ([type]): &lt;=workspace.time_size(): the number of timesteps to keep

        Returns:
            [Workspace]: The resulting workspace
        &#34;&#34;&#34;
        b = self.batch_size()
        t = self.time_size()
        to_aggregate = []
        for _ in range(n_times):
            assert not n_timesteps &gt; t
            mini_workspace = self
            if n_timesteps &lt; t:
                t = np.random.randint(t - n_timesteps)
                mini_workspace = self.subtime(t, t + n_timesteps)

            # Batch sampling
            if n_batch_elements &lt; b:
                idx_envs = torch.randperm(b)[:n_batch_elements]
                mini_workspace = mini_workspace.select_batch(idx_envs)
            to_aggregate.append(mini_workspace)

        if len(to_aggregate) &gt; 1:
            mini_workspace = Workspace.cat_batch(to_aggregate)
        else:
            mini_workspace = to_aggregate[0]
        return mini_workspace

    def get_transitions(self) -&gt; Workspace:
        &#34;&#34;&#34;
        Takes in a workspace from salina:
        [(step1),(step2),(step3), ... ]
        return a workspace of transitions :
        [
            [step1,step2],
            [step2,step3]
            ...
        ]
        Filters every transitions [step_final,step_initial]
        &#34;&#34;&#34;
        transitions = {}
        done = self[&#34;env/done&#34;][:-1]
        for key in self.keys():
            array = self[key]

            # remove transitions (s_terminal -&gt; s_initial)
            x = array[:-1][~done]
            x_next = array[1:][~done]
            transitions[key] = torch.stack([x, x_next])

        workspace = Workspace()
        for k, v in transitions.items():
            workspace.set_full(k, v)
        return workspace

    def debug_transitions(self, truncated):
        &#34;&#34;&#34; &#34;&#34;&#34;
        critic, done, action_probs, reward, action = self[
            &#34;critic&#34;, &#34;env/done&#34;, &#34;action_probs&#34;, &#34;env/reward&#34;, &#34;action&#34;
        ]
        timestep = self[&#34;env/timestep&#34;]
        assert not done[
            0
        ].max()  # dones is must be always false in the first timestep of the transition.
        # if not it means we have a transition (step final) =&gt; (step initial)

        # timesteps must always follow each other.
        assert (timestep[0] == timestep[1] - 1).all()

        assert (
            truncated[not done].sum().item() == 0
        )  # when done is false, truncated is always false

        if done[truncated].numel() &gt; 0:
            assert torch.amin(
                done[truncated]
            )  # when truncated is true, done is always true
        assert reward[1].sum() == len(
            reward[1]
        ), &#34;in cartpole, rewards are always 1&#34;  # only 1 rewards


class _SplitSharedWorkspace:
    &#34;&#34;&#34;This is a view over a Workspace, restricted to particular batch dimensions.
    It is used when multiple agents are reading/writing in the same workspace
    but for specific batch dimensions (see NRemoteAgent)
    &#34;&#34;&#34;

    def __init__(self, workspace, batch_dims):
        self.workspace = workspace
        self.batch_dims = batch_dims
        self.is_shared = self.workspace.is_shared

    def set(self, var_name, t, v):
        self.workspace.set(var_name, t, v, batch_dims=self.batch_dims)

    def get(self, var_name, t):
        return self.workspace.get(var_name, t, batch_dims=self.batch_dims)

    def keys(self):
        return self.workspace.keys()

    def get_time_truncated(self, var_name, from_time, to_time):
        assert 0 &lt;= from_time &lt; to_time
        return self.workspace.get_time_truncated(
            var_name, from_time, to_time, batch_dims=self.batch_dims
        )

    def set_full(self, var_name, value):
        self.workspace.set_full(var_name, value, batch_dims=self.batch_dims)

    def get_full(self, var_name):
        return self.workspace.get_full(var_name, batch_dims=self.batch_dims)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="bbrl.workspace.take_per_row_strided"><code class="name flex">
<span>def <span class="ident">take_per_row_strided</span></span>(<span>a, index, num_elem=2)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def take_per_row_strided(a, index, num_elem=2):
    # TODO: Optimize this function
    all_index = index
    arange = torch.arange(a.size()[1], device=a.device)
    return torch.cat(
        [a[all_index + t, arange].unsqueeze(0) for t in range(num_elem)], dim=0
    )</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="bbrl.workspace.CompactSharedTensor"><code class="flex name class">
<span>class <span class="ident">CompactSharedTensor</span></span>
<span>(</span><span>_tensor: torch.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>It corresponds to a tensor in shared memory.
It is used when building a workspace shared by multiple processes.
All the methods behaves like the methods of <code><a title="bbrl.workspace.SlicedTemporalTensor" href="#bbrl.workspace.SlicedTemporalTensor">SlicedTemporalTensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CompactSharedTensor:
    &#34;&#34;&#34;It corresponds to a tensor in shared memory.
    It is used when building a workspace shared by multiple processes.
        All the methods behaves like the methods of `SlicedTemporalTensor`
    &#34;&#34;&#34;

    def __init__(self, _tensor: torch.Tensor):
        self.tensor = _tensor
        self.tensor.share_memory_()

    def set(self, t, value, batch_dims):
        if batch_dims is None:
            self.tensor[t] = value.detach()
        else:
            self.tensor[t, batch_dims[0] : batch_dims[1]] = value.detach()

    def get(self, t, batch_dims):
        assert t &lt; self.tensor.size()[0], &#34;Temporal index out of bounds&#34;
        if batch_dims is None:
            return self.tensor[t]
        else:
            return self.tensor[t, batch_dims[0] : batch_dims[1]]

    def to(self, device):
        if device == self.tensor.device:
            return self
        t = self.tensor.to(device)
        t.share_memory_()
        return CompactSharedTensor(t)

    def select_batch(self, batch_indexes):
        v = CompactSharedTensor(self.tensor[:, batch_indexes])
        return v

    def get_full(self, batch_dims):
        if batch_dims is None:
            return self.tensor
        else:
            return self.tensor[:, batch_dims[0] : batch_dims[1]]

    def time_size(self):
        return self.tensor.size()[0]

    def batch_size(self):
        return self.tensor.size()[1]

    def set_full(self, value, batch_dims):
        if batch_dims is None:
            self.tensor = value.detach()
        else:
            self.tensor[:, batch_dims[0] : batch_dims[1]] = value.detach()

    def clear(self):
        assert False, &#34;Cannot clear a shared tensor&#34;

    def subtime(self, from_t, to_t):
        t = self.tensor[from_t:to_t]
        return CompactSharedTensor(t)

    def copy_time(self, from_time, to_time, n_steps):
        self.tensor[to_time : to_time + n_steps] = self.tensor[
            from_time : from_time + n_steps
        ]

    def zero_grad(self):
        pass</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="bbrl.workspace.CompactSharedTensor.batch_size"><code class="name flex">
<span>def <span class="ident">batch_size</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch_size(self):
    return self.tensor.size()[1]</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactSharedTensor.clear"><code class="name flex">
<span>def <span class="ident">clear</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clear(self):
    assert False, &#34;Cannot clear a shared tensor&#34;</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactSharedTensor.copy_time"><code class="name flex">
<span>def <span class="ident">copy_time</span></span>(<span>self, from_time, to_time, n_steps)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy_time(self, from_time, to_time, n_steps):
    self.tensor[to_time : to_time + n_steps] = self.tensor[
        from_time : from_time + n_steps
    ]</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactSharedTensor.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self, t, batch_dims)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(self, t, batch_dims):
    assert t &lt; self.tensor.size()[0], &#34;Temporal index out of bounds&#34;
    if batch_dims is None:
        return self.tensor[t]
    else:
        return self.tensor[t, batch_dims[0] : batch_dims[1]]</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactSharedTensor.get_full"><code class="name flex">
<span>def <span class="ident">get_full</span></span>(<span>self, batch_dims)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_full(self, batch_dims):
    if batch_dims is None:
        return self.tensor
    else:
        return self.tensor[:, batch_dims[0] : batch_dims[1]]</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactSharedTensor.select_batch"><code class="name flex">
<span>def <span class="ident">select_batch</span></span>(<span>self, batch_indexes)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_batch(self, batch_indexes):
    v = CompactSharedTensor(self.tensor[:, batch_indexes])
    return v</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactSharedTensor.set"><code class="name flex">
<span>def <span class="ident">set</span></span>(<span>self, t, value, batch_dims)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set(self, t, value, batch_dims):
    if batch_dims is None:
        self.tensor[t] = value.detach()
    else:
        self.tensor[t, batch_dims[0] : batch_dims[1]] = value.detach()</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactSharedTensor.set_full"><code class="name flex">
<span>def <span class="ident">set_full</span></span>(<span>self, value, batch_dims)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_full(self, value, batch_dims):
    if batch_dims is None:
        self.tensor = value.detach()
    else:
        self.tensor[:, batch_dims[0] : batch_dims[1]] = value.detach()</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactSharedTensor.subtime"><code class="name flex">
<span>def <span class="ident">subtime</span></span>(<span>self, from_t, to_t)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def subtime(self, from_t, to_t):
    t = self.tensor[from_t:to_t]
    return CompactSharedTensor(t)</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactSharedTensor.time_size"><code class="name flex">
<span>def <span class="ident">time_size</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def time_size(self):
    return self.tensor.size()[0]</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactSharedTensor.to"><code class="name flex">
<span>def <span class="ident">to</span></span>(<span>self, device)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to(self, device):
    if device == self.tensor.device:
        return self
    t = self.tensor.to(device)
    t.share_memory_()
    return CompactSharedTensor(t)</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactSharedTensor.zero_grad"><code class="name flex">
<span>def <span class="ident">zero_grad</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zero_grad(self):
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bbrl.workspace.CompactTemporalTensor"><code class="flex name class">
<span>class <span class="ident">CompactTemporalTensor</span></span>
<span>(</span><span>value: torch.Tensor = None)</span>
</code></dt>
<dd>
<div class="desc"><p>A CompactTemporalTensor is a tensor of size TxBx&hellip;
It behaves like the <code><a title="bbrl.workspace.SlicedTemporalTensor" href="#bbrl.workspace.SlicedTemporalTensor">SlicedTemporalTensor</a></code> but has a fixed size that cannot change.
It is faster than the SlicedTemporalTensor.
See <code><a title="bbrl.workspace.SlicedTemporalTensor" href="#bbrl.workspace.SlicedTemporalTensor">SlicedTemporalTensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CompactTemporalTensor:
    &#34;&#34;&#34;A CompactTemporalTensor is a tensor of size TxBx...
    It behaves like the `SlicedTemporalTensor` but has a fixed size that cannot change.
    It is faster than the SlicedTemporalTensor.
        See `SlicedTemporalTensor`
    &#34;&#34;&#34;

    def __init__(self, value: torch.Tensor = None):
        self.size = None
        self.device = None
        self.dtype = None
        self.tensor = None
        if value is not None:
            self.tensor = value
            self.device = value.device
            self.size = value.size()
            self.dtype = value.dtype

    def set(self, t, value, batch_dims):
        assert False
        assert self.tensor is not None, &#34;Tensor must be initialized&#34;
        assert self.size[1:] == value.size(), &#34;Incompatible size&#34;
        assert self.device == value.device, &#34;Incompatible device&#34;
        assert self.dtype == value.dtype, &#34;Incompatible type&#34;
        assert t &lt; self.tensor.size()[0], &#34;Temporal index out of bounds&#34;
        if batch_dims is None:
            self.tensor[t] = value
        else:
            self.tensor[t, batch_dims[0] : batch_dims[1]] = value

    def select_batch(self, batch_indexes):
        v = CompactTemporalTensor(self.tensor[:, batch_indexes])
        return v

    def to_sliced(self) -&gt; SlicedTemporalTensor:
        &#34;&#34;&#34;Transform the tensor to a `SlicedTemporalTensor`&#34;&#34;&#34;
        v = SlicedTemporalTensor()
        for t in range(self.tensor.size()[0]):
            v.set(t, self.tensor[t], None)
        return v

    def to(self, device):
        if device == self.tensor.device:
            return self
        t = self.tensor.to(device)
        return CompactTemporalTensor(t)

    def get(self, t, batch_dims):
        assert t &lt; self.tensor.size()[0], &#34;Temporal index out of bounds&#34;
        if batch_dims is None:
            return self.tensor[t]
        else:
            return self.tensor[t, batch_dims[0] : batch_dims[1]]

    def get_full(self, batch_dims):
        if batch_dims is None:
            return self.tensor
        else:
            return self.tensor[:, batch_dims[0] : batch_dims[1]]

    def time_size(self):
        return self.tensor.size()[0]

    def batch_size(self):
        return self.tensor.size()[1]

    def set_full(self, value, batch_dims):
        if self.tensor is None:
            assert batch_dims is None
            self.size = value.size()
            self.dtype = value.dtype
            self.device = value.device
        if batch_dims is None:
            self.tensor = value
        else:
            self.tensor[:, batch_dims[0] : batch_dims[1]] = value

    def subtime(self, from_t, to_t):
        return CompactTemporalTensor(self.tensor[from_t:to_t])

    def clear(self):
        self.size = None
        self.device = None
        self.dtype = None
        self.tensor = None

    def copy_time(self, from_time, to_time, n_steps):
        self.tensor[to_time : to_time + n_steps] = self.tensor[
            from_time : from_time + n_steps
        ]

    def zero_grad(self):
        self.tensor = self.tensor.detach()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="bbrl.workspace.CompactTemporalTensor.batch_size"><code class="name flex">
<span>def <span class="ident">batch_size</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch_size(self):
    return self.tensor.size()[1]</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactTemporalTensor.clear"><code class="name flex">
<span>def <span class="ident">clear</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clear(self):
    self.size = None
    self.device = None
    self.dtype = None
    self.tensor = None</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactTemporalTensor.copy_time"><code class="name flex">
<span>def <span class="ident">copy_time</span></span>(<span>self, from_time, to_time, n_steps)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy_time(self, from_time, to_time, n_steps):
    self.tensor[to_time : to_time + n_steps] = self.tensor[
        from_time : from_time + n_steps
    ]</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactTemporalTensor.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self, t, batch_dims)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(self, t, batch_dims):
    assert t &lt; self.tensor.size()[0], &#34;Temporal index out of bounds&#34;
    if batch_dims is None:
        return self.tensor[t]
    else:
        return self.tensor[t, batch_dims[0] : batch_dims[1]]</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactTemporalTensor.get_full"><code class="name flex">
<span>def <span class="ident">get_full</span></span>(<span>self, batch_dims)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_full(self, batch_dims):
    if batch_dims is None:
        return self.tensor
    else:
        return self.tensor[:, batch_dims[0] : batch_dims[1]]</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactTemporalTensor.select_batch"><code class="name flex">
<span>def <span class="ident">select_batch</span></span>(<span>self, batch_indexes)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_batch(self, batch_indexes):
    v = CompactTemporalTensor(self.tensor[:, batch_indexes])
    return v</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactTemporalTensor.set"><code class="name flex">
<span>def <span class="ident">set</span></span>(<span>self, t, value, batch_dims)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set(self, t, value, batch_dims):
    assert False
    assert self.tensor is not None, &#34;Tensor must be initialized&#34;
    assert self.size[1:] == value.size(), &#34;Incompatible size&#34;
    assert self.device == value.device, &#34;Incompatible device&#34;
    assert self.dtype == value.dtype, &#34;Incompatible type&#34;
    assert t &lt; self.tensor.size()[0], &#34;Temporal index out of bounds&#34;
    if batch_dims is None:
        self.tensor[t] = value
    else:
        self.tensor[t, batch_dims[0] : batch_dims[1]] = value</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactTemporalTensor.set_full"><code class="name flex">
<span>def <span class="ident">set_full</span></span>(<span>self, value, batch_dims)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_full(self, value, batch_dims):
    if self.tensor is None:
        assert batch_dims is None
        self.size = value.size()
        self.dtype = value.dtype
        self.device = value.device
    if batch_dims is None:
        self.tensor = value
    else:
        self.tensor[:, batch_dims[0] : batch_dims[1]] = value</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactTemporalTensor.subtime"><code class="name flex">
<span>def <span class="ident">subtime</span></span>(<span>self, from_t, to_t)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def subtime(self, from_t, to_t):
    return CompactTemporalTensor(self.tensor[from_t:to_t])</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactTemporalTensor.time_size"><code class="name flex">
<span>def <span class="ident">time_size</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def time_size(self):
    return self.tensor.size()[0]</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactTemporalTensor.to"><code class="name flex">
<span>def <span class="ident">to</span></span>(<span>self, device)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to(self, device):
    if device == self.tensor.device:
        return self
    t = self.tensor.to(device)
    return CompactTemporalTensor(t)</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactTemporalTensor.to_sliced"><code class="name flex">
<span>def <span class="ident">to_sliced</span></span>(<span>self) ‑> <a title="bbrl.workspace.SlicedTemporalTensor" href="#bbrl.workspace.SlicedTemporalTensor">SlicedTemporalTensor</a></span>
</code></dt>
<dd>
<div class="desc"><p>Transform the tensor to a <code><a title="bbrl.workspace.SlicedTemporalTensor" href="#bbrl.workspace.SlicedTemporalTensor">SlicedTemporalTensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_sliced(self) -&gt; SlicedTemporalTensor:
    &#34;&#34;&#34;Transform the tensor to a `SlicedTemporalTensor`&#34;&#34;&#34;
    v = SlicedTemporalTensor()
    for t in range(self.tensor.size()[0]):
        v.set(t, self.tensor[t], None)
    return v</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.CompactTemporalTensor.zero_grad"><code class="name flex">
<span>def <span class="ident">zero_grad</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zero_grad(self):
    self.tensor = self.tensor.detach()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bbrl.workspace.SlicedTemporalTensor"><code class="flex name class">
<span>class <span class="ident">SlicedTemporalTensor</span></span>
</code></dt>
<dd>
<div class="desc"><p>A SlicedTemporalTensor represents a tensor of size TxBx&hellip; by using a list of tensors of size Bx&hellip;
The interest is that this tensor automatically adapts its timestep dimension and does not need to have a predefined size.</p>
<p>Initialize an empty tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SlicedTemporalTensor:
    &#34;&#34;&#34;A SlicedTemporalTensor represents a tensor of size TxBx... by using a list of tensors of size Bx...
    The interest is that this tensor automatically adapts its timestep dimension and does not need to have a predefined size.
    &#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;Initialize an empty tensor&#34;&#34;&#34;
        self.tensors: list[torch.Tensor] = []
        self.size: torch.Size = None
        self.device: torch.device = None
        self.dtype: torch.dtype = None

    def set(self, t: int, value: torch.Tensor, batch_dims: Optional[tuple[int, int]]):
        &#34;&#34;&#34;Set a value (dim Bx...) at time t&#34;&#34;&#34;
        assert (
            batch_dims is None
        ), &#34;Unable to use batch dimensions with SlicedTemporalTensor&#34;
        if self.size is None:
            self.size = value.size()
            self.device = value.device
            self.dtype = value.dtype
        assert self.size == value.size(), (
            &#34;Incompatible size:&#34; + str(self.size) + &#34; vs &#34; + str(value.size())
        )
        assert self.device == value.device, &#34;Incompatible device&#34;
        assert self.dtype == value.dtype, &#34;Incompatible type&#34;
        while len(self.tensors) &lt;= t:
            self.tensors.append(
                torch.zeros(*self.size, device=self.device, dtype=self.dtype)
            )
        self.tensors[t] = value

    def to(self, device: torch.device):
        &#34;&#34;&#34;Move the tensor to a specific device&#34;&#34;&#34;
        s = SlicedTemporalTensor()
        for k in range(len(self.tensors)):
            s.set(k, self.tensors[k].to(device))
        return s

    def get(self, t: int, batch_dims: Optional[tuple[int, int]]):
        &#34;&#34;&#34;Get the value of the tensor at time t&#34;&#34;&#34;

        assert (
            batch_dims is None
        ), &#34;Unable to use batch dimensions with SlicedTemporalTensor&#34;
        assert t &lt; len(self.tensors), &#34;Temporal index out of bounds&#34;
        return self.tensors[t]

    def get_full(self, batch_dims):
        &#34;&#34;&#34;Returns the complete tensor of size TxBx...&#34;&#34;&#34;

        assert (
            batch_dims is None
        ), &#34;Unable to use batch dimensions with SlicedTemporalTensor&#34;
        return torch.cat([a.unsqueeze(0) for a in self.tensors], dim=0)

    def get_time_truncated(
        self, from_time: int, to_time: int, batch_dims: Optional[tuple[int, int]]
    ):
        &#34;&#34;&#34;Returns tensor[from_time:to_time]&#34;&#34;&#34;
        assert from_time &gt;= 0 and to_time &gt;= 0 and to_time &gt; from_time
        assert batch_dims is None
        return torch.cat(
            [
                self.tensors[k].unsqueeze(0)
                for k in range(from_time, min(len(self.tensors), to_time))
            ],
            dim=0,
        )

    def set_full(self, value: torch.Tensor, batch_dims: Optional[tuple[int, int]]):
        &#34;&#34;&#34;Set the tensor given a BxTx... tensor.
        The input tensor is cut into slices that are stored in a list of tensors
        &#34;&#34;&#34;
        assert (
            batch_dims is None
        ), &#34;Unable to use batch dimensions with SlicedTemporalTensor&#34;
        for t in range(value.size()[0]):
            self.set(t, value[t], batch_dims=batch_dims)

    def time_size(self):
        &#34;&#34;&#34;
        Return the size of the time dimension
        &#34;&#34;&#34;
        return len(self.tensors)

    def batch_size(self):
        &#34;&#34;&#34;Return the size of the batch dimension&#34;&#34;&#34;
        return self.tensors[0].size()[0]

    def select_batch(self, batch_indexes: torch.LongTensor):
        &#34;&#34;&#34;Return the tensor where the batch dimension has been selected by the index&#34;&#34;&#34;
        var = SlicedTemporalTensor()
        for t, v in enumerate(self.tensors):
            batch_indexes = batch_indexes.to(v.device)
            var.set(t, v[batch_indexes], None)
        return var

    def clear(self):
        &#34;&#34;&#34;Clear the tensor&#34;&#34;&#34;
        self.tensors = []
        self.size = None
        self.device = None
        self.dtype = None

    def copy_time(self, from_time: int, to_time: int, n_steps: int):
        &#34;&#34;&#34;Copy temporal slices of the tensor from from_time:from_time+n_steps to to_time:to_time+n_steps&#34;&#34;&#34;
        for t in range(n_steps):
            v = self.get(from_time + t, batch_dims=None)
            self.set(to_time + t, v, batch_dims=None)

    def subtime(self, from_t: int, to_t: int):
        &#34;&#34;&#34;
        Return tensor[from_t:to_t]

        &#34;&#34;&#34;
        return CompactTemporalTensor(
            torch.cat([a.unsqueeze(0) for a in self.tensors[from_t:to_t]], dim=0)
        )

    def zero_grad(self):
        &#34;&#34;&#34;Clear any gradient information in the tensor&#34;&#34;&#34;
        self.tensors = [v.detach() for v in self.tensors]</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="bbrl.workspace.SlicedTemporalTensor.batch_size"><code class="name flex">
<span>def <span class="ident">batch_size</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the size of the batch dimension</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch_size(self):
    &#34;&#34;&#34;Return the size of the batch dimension&#34;&#34;&#34;
    return self.tensors[0].size()[0]</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.SlicedTemporalTensor.clear"><code class="name flex">
<span>def <span class="ident">clear</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Clear the tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clear(self):
    &#34;&#34;&#34;Clear the tensor&#34;&#34;&#34;
    self.tensors = []
    self.size = None
    self.device = None
    self.dtype = None</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.SlicedTemporalTensor.copy_time"><code class="name flex">
<span>def <span class="ident">copy_time</span></span>(<span>self, from_time: int, to_time: int, n_steps: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Copy temporal slices of the tensor from from_time:from_time+n_steps to to_time:to_time+n_steps</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy_time(self, from_time: int, to_time: int, n_steps: int):
    &#34;&#34;&#34;Copy temporal slices of the tensor from from_time:from_time+n_steps to to_time:to_time+n_steps&#34;&#34;&#34;
    for t in range(n_steps):
        v = self.get(from_time + t, batch_dims=None)
        self.set(to_time + t, v, batch_dims=None)</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.SlicedTemporalTensor.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self, t: int, batch_dims: Optional[tuple[int, int]])</span>
</code></dt>
<dd>
<div class="desc"><p>Get the value of the tensor at time t</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(self, t: int, batch_dims: Optional[tuple[int, int]]):
    &#34;&#34;&#34;Get the value of the tensor at time t&#34;&#34;&#34;

    assert (
        batch_dims is None
    ), &#34;Unable to use batch dimensions with SlicedTemporalTensor&#34;
    assert t &lt; len(self.tensors), &#34;Temporal index out of bounds&#34;
    return self.tensors[t]</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.SlicedTemporalTensor.get_full"><code class="name flex">
<span>def <span class="ident">get_full</span></span>(<span>self, batch_dims)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the complete tensor of size TxBx&hellip;</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_full(self, batch_dims):
    &#34;&#34;&#34;Returns the complete tensor of size TxBx...&#34;&#34;&#34;

    assert (
        batch_dims is None
    ), &#34;Unable to use batch dimensions with SlicedTemporalTensor&#34;
    return torch.cat([a.unsqueeze(0) for a in self.tensors], dim=0)</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.SlicedTemporalTensor.get_time_truncated"><code class="name flex">
<span>def <span class="ident">get_time_truncated</span></span>(<span>self, from_time: int, to_time: int, batch_dims: Optional[tuple[int, int]])</span>
</code></dt>
<dd>
<div class="desc"><p>Returns tensor[from_time:to_time]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_time_truncated(
    self, from_time: int, to_time: int, batch_dims: Optional[tuple[int, int]]
):
    &#34;&#34;&#34;Returns tensor[from_time:to_time]&#34;&#34;&#34;
    assert from_time &gt;= 0 and to_time &gt;= 0 and to_time &gt; from_time
    assert batch_dims is None
    return torch.cat(
        [
            self.tensors[k].unsqueeze(0)
            for k in range(from_time, min(len(self.tensors), to_time))
        ],
        dim=0,
    )</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.SlicedTemporalTensor.select_batch"><code class="name flex">
<span>def <span class="ident">select_batch</span></span>(<span>self, batch_indexes: torch.LongTensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the tensor where the batch dimension has been selected by the index</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_batch(self, batch_indexes: torch.LongTensor):
    &#34;&#34;&#34;Return the tensor where the batch dimension has been selected by the index&#34;&#34;&#34;
    var = SlicedTemporalTensor()
    for t, v in enumerate(self.tensors):
        batch_indexes = batch_indexes.to(v.device)
        var.set(t, v[batch_indexes], None)
    return var</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.SlicedTemporalTensor.set"><code class="name flex">
<span>def <span class="ident">set</span></span>(<span>self, t: int, value: torch.Tensor, batch_dims: Optional[tuple[int, int]])</span>
</code></dt>
<dd>
<div class="desc"><p>Set a value (dim Bx&hellip;) at time t</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set(self, t: int, value: torch.Tensor, batch_dims: Optional[tuple[int, int]]):
    &#34;&#34;&#34;Set a value (dim Bx...) at time t&#34;&#34;&#34;
    assert (
        batch_dims is None
    ), &#34;Unable to use batch dimensions with SlicedTemporalTensor&#34;
    if self.size is None:
        self.size = value.size()
        self.device = value.device
        self.dtype = value.dtype
    assert self.size == value.size(), (
        &#34;Incompatible size:&#34; + str(self.size) + &#34; vs &#34; + str(value.size())
    )
    assert self.device == value.device, &#34;Incompatible device&#34;
    assert self.dtype == value.dtype, &#34;Incompatible type&#34;
    while len(self.tensors) &lt;= t:
        self.tensors.append(
            torch.zeros(*self.size, device=self.device, dtype=self.dtype)
        )
    self.tensors[t] = value</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.SlicedTemporalTensor.set_full"><code class="name flex">
<span>def <span class="ident">set_full</span></span>(<span>self, value: torch.Tensor, batch_dims: Optional[tuple[int, int]])</span>
</code></dt>
<dd>
<div class="desc"><p>Set the tensor given a BxTx&hellip; tensor.
The input tensor is cut into slices that are stored in a list of tensors</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_full(self, value: torch.Tensor, batch_dims: Optional[tuple[int, int]]):
    &#34;&#34;&#34;Set the tensor given a BxTx... tensor.
    The input tensor is cut into slices that are stored in a list of tensors
    &#34;&#34;&#34;
    assert (
        batch_dims is None
    ), &#34;Unable to use batch dimensions with SlicedTemporalTensor&#34;
    for t in range(value.size()[0]):
        self.set(t, value[t], batch_dims=batch_dims)</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.SlicedTemporalTensor.subtime"><code class="name flex">
<span>def <span class="ident">subtime</span></span>(<span>self, from_t: int, to_t: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Return tensor[from_t:to_t]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def subtime(self, from_t: int, to_t: int):
    &#34;&#34;&#34;
    Return tensor[from_t:to_t]

    &#34;&#34;&#34;
    return CompactTemporalTensor(
        torch.cat([a.unsqueeze(0) for a in self.tensors[from_t:to_t]], dim=0)
    )</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.SlicedTemporalTensor.time_size"><code class="name flex">
<span>def <span class="ident">time_size</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the size of the time dimension</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def time_size(self):
    &#34;&#34;&#34;
    Return the size of the time dimension
    &#34;&#34;&#34;
    return len(self.tensors)</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.SlicedTemporalTensor.to"><code class="name flex">
<span>def <span class="ident">to</span></span>(<span>self, device: torch.device)</span>
</code></dt>
<dd>
<div class="desc"><p>Move the tensor to a specific device</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to(self, device: torch.device):
    &#34;&#34;&#34;Move the tensor to a specific device&#34;&#34;&#34;
    s = SlicedTemporalTensor()
    for k in range(len(self.tensors)):
        s.set(k, self.tensors[k].to(device))
    return s</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.SlicedTemporalTensor.zero_grad"><code class="name flex">
<span>def <span class="ident">zero_grad</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Clear any gradient information in the tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zero_grad(self):
    &#34;&#34;&#34;Clear any gradient information in the tensor&#34;&#34;&#34;
    self.tensors = [v.detach() for v in self.tensors]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bbrl.workspace.Workspace"><code class="flex name class">
<span>class <span class="ident">Workspace</span></span>
<span>(</span><span>workspace: Optional[<a title="bbrl.workspace.Workspace" href="#bbrl.workspace.Workspace">Workspace</a>] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Workspace is the most important class in <code><a title="bbrl" href="index.html">bbrl</a></code>.
It correponds to a collection of tensors
('SlicedTemporalTensor<code>, </code>CompactTemporalTensor<code> or </code> CompactSharedTensor`).
In the majority of cases, we consider that all the tensors have the same time and batch sizes
(but it is not mandatory for most of the functions)</p>
<p>Create an empty workspace</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>workspace</code></strong> :&ensp;<code><a title="bbrl.workspace.Workspace" href="#bbrl.workspace.Workspace">Workspace</a></code>, optional</dt>
<dd>If specified, it creates a copy of the workspace</dd>
</dl>
<p>(where tensors are cloned as CompactTemporalTensors)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Workspace:
    &#34;&#34;&#34;Workspace is the most important class in `bbrl`.
    It correponds to a collection of tensors
    (&#39;SlicedTemporalTensor`, `CompactTemporalTensor` or ` CompactSharedTensor`).
    In the majority of cases, we consider that all the tensors have the same time and batch sizes
    (but it is not mandatory for most of the functions)
    &#34;&#34;&#34;

    def __init__(self, workspace: Optional[Workspace] = None):
        &#34;&#34;&#34;Create an empty workspace

        Args:
            workspace (Workspace, optional): If specified, it creates a copy of the workspace
        (where tensors are cloned as CompactTemporalTensors)
        &#34;&#34;&#34;
        self.variables = {}
        self.is_shared = False
        if workspace is not None:
            for k in workspace.keys():
                self.set_full(k, workspace[k].clone())

    def set(
        self,
        var_name: str,
        t: int,
        v: torch.Tensor,
        batch_dims: Optional[tuple[int, int]] = None,
    ):
        &#34;&#34;&#34;Set the variable var_name at time t&#34;&#34;&#34;
        if var_name not in self.variables:
            assert not self.is_shared, &#34;Cannot add new variable into a shared workspace&#34;
            self.variables[var_name] = SlicedTemporalTensor()
        elif isinstance(self.variables[var_name], CompactTemporalTensor):
            self.variables[var_name] = self.variables[var_name].to_sliced()

        self.variables[var_name].set(t, v, batch_dims=batch_dims)

    def get(
        self, var_name: str, t: int, batch_dims: Optional[tuple[int, int]] = None
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;Get the variable var_name at time t&#34;&#34;&#34;
        assert var_name in self.variables, &#34;Unknown variable &#39;&#34; + var_name + &#34;&#39;&#34;
        return self.variables[var_name].get(t, batch_dims=batch_dims)

    def clear(self, name=None):
        &#34;&#34;&#34;Remove all the variables from the workspace&#34;&#34;&#34;
        if name is None:
            for k, v in self.variables.items():
                v.clear()
            self.variables = {}
        else:
            self.variables[name].clear()
            del self.variables[name]

    def contiguous(self) -&gt; Workspace:
        &#34;&#34;&#34;Generates a workspace where all tensors are stored in the Compact format.&#34;&#34;&#34;
        workspace = Workspace()
        for k in self.keys():
            workspace.set_full(k, self.get_full(k))
        return workspace

    def set_full(
        self,
        var_name: str,
        value: torch.Tensor,
        batch_dims: Optional[tuple[int, int]] = None,
    ):
        &#34;&#34;&#34;Set variable var_name with a complete tensor (TxBx...) where T is the time dimension
        and B is the batch size
        &#34;&#34;&#34;
        if var_name not in self.variables:
            assert not self.is_shared, &#34;Cannot add new variable into a shared workspace&#34;
            self.variables[var_name] = CompactTemporalTensor()
        self.variables[var_name].set_full(value, batch_dims=batch_dims)

    def get_full(
        self, var_name: str, batch_dims: Optional[tuple[int, int]] = None
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;Return the complete tensor for var_name&#34;&#34;&#34;
        assert var_name in self.variables, (
            &#34;[Workspace.get_full] unknown variable &#39;&#34; + var_name + &#34;&#39;&#34;
        )
        return self.variables[var_name].get_full(batch_dims=batch_dims)

    def keys(self):
        &#34;&#34;&#34;Return an iterator over the variables names&#34;&#34;&#34;
        return self.variables.keys()

    def __getitem__(self, key):
        &#34;&#34;&#34;If key is a string, then it returns a torch.Tensor
        If key is a list of string, it returns a tuple of torch.Tensor
        &#34;&#34;&#34;
        if isinstance(key, str):
            return self.get_full(key, None)
        else:
            return (self.get_full(k, None) for k in key)

    def _all_variables_same_time_size(self) -&gt; bool:
        &#34;&#34;&#34;Check that all variables have the same time size&#34;&#34;&#34;
        _ts = None
        for k, v in self.variables.items():
            if _ts is None:
                _ts = v.time_size()
            if _ts != v.time_size():
                return False
        return True

    def time_size(self) -&gt; int:
        &#34;&#34;&#34;Return the time size of the variables in the workspace&#34;&#34;&#34;
        _ts = None
        for k, v in self.variables.items():
            if _ts is None:
                _ts = v.time_size()
            assert _ts == v.time_size(), &#34;Variables must have the same time size&#34;
        return _ts

    def batch_size(self) -&gt; int:
        &#34;&#34;&#34;Return the batch size of the variables in the workspace&#34;&#34;&#34;
        _bs = None
        for k, v in self.variables.items():
            if _bs is None:
                _bs = v.batch_size()
            assert _bs == v.batch_size(), &#34;Variables must have the same batch size&#34;
        return _bs

    def select_batch(self, batch_indexes: torch.LongTensor) -&gt; Workspace:
        &#34;&#34;&#34;Given a tensor of indexes, returns a new workspace
        with the selected elements (over the batch dimension)
        &#34;&#34;&#34;
        _bs = None
        for k, v in self.variables.items():
            if _bs is None:
                _bs = v.batch_size()
            assert _bs == v.batch_size(), &#34;Variables must have the same batch size&#34;

        workspace = Workspace()
        for k, v in self.variables.items():
            v = v.select_batch(batch_indexes)
            workspace.variables[k] = v
        return workspace

    def select_batch_n(self, n):
        &#34;&#34;&#34;Return a new Workspace of batch_size==n by randomly sampling over the batch dimensions&#34;&#34;&#34;
        who = torch.randint(low=0, high=self.batch_size(), size=(n,))
        return self.select_batch(who)

    def copy_time(
        self,
        from_time: int,
        to_time: int,
        n_steps: int,
        var_names: Optional[list[str]] = None,
    ):
        &#34;&#34;&#34;Copy all the variables values from time `from_time` to `from_time+n_steps`
        to `to_time` to `to_time+n_steps`
        It can be restricted to specific variables using `var_names`.
        &#34;&#34;&#34;
        for k, v in self.variables.items():
            if var_names is None or k in var_names:
                v.copy_time(from_time, to_time, n_steps)

    def get_time_truncated(
        self,
        var_name: str,
        from_time: int,
        to_time: int,
        batch_dims: Optional[tuple[int, int]] = None,
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;Return workspace[var_name][from_time:to_time]&#34;&#34;&#34;
        assert from_time &gt;= 0 and to_time &gt;= 0 and to_time &gt; from_time

        v = self.variables[var_name]
        if isinstance(v, SlicedTemporalTensor):
            return v.get_time_truncated(from_time, to_time, batch_dims)
        else:
            return v.get_full(batch_dims)[from_time:to_time]

    def get_time_truncated_workspace(self, from_time: int, to_time: int) -&gt; Workspace:
        &#34;&#34;&#34;Return a workspace where all variables are truncated between from_time and to_time&#34;&#34;&#34;
        workspace = Workspace()
        for k in self.keys():
            workspace.set_full(k, self.get_time_truncated(k, from_time, to_time, None))
        return workspace

    # Static function
    def cat_batch(self, workspaces: list[Workspace]) -&gt; Workspace:
        &#34;&#34;&#34;Concatenate multiple workspaces over the batch dimension.
        The workspaces must have the same time dimension.
        &#34;&#34;&#34;

        ts = None
        for w in workspaces:
            if ts is None:
                ts = w.time_size()
            assert ts == w.time_size(), &#34;Workspaces must have the same time size&#34;

        workspace = Workspace()
        for k in workspaces[0].keys():
            vals = [w[k] for w in workspaces]
            v = torch.cat(vals, dim=1)
            workspace.set_full(k, v)
        return workspace

    def copy_n_last_steps(self, n: int, var_names: Optional[list[str]] = None) -&gt; None:
        &#34;&#34;&#34;Copy the n last timesteps of each variable to the n first timesteps.&#34;&#34;&#34;
        _ts = None
        for k, v in self.variables.items():
            if var_names is None or k in var_names:
                if _ts is None:
                    _ts = v.time_size()
                assert _ts == v.time_size(), (
                    &#34;Variables must have the same time size: &#34;
                    + str(_ts)
                    + &#34; vs &#34;
                    + str(v.time_size())
                )

        for k, v in self.variables.items():
            if var_names is None or k in var_names:
                self.copy_time(_ts - n, 0, n)

    def zero_grad(self) -&gt; None:
        &#34;&#34;&#34;Remove any gradient information&#34;&#34;&#34;
        for k, v in self.variables.items():
            v.zero_grad()

    def to(self, device: torch.device) -&gt; Workspace:
        &#34;&#34;&#34;Return a workspace where all tensors are on a particular device&#34;&#34;&#34;
        workspace = Workspace()
        for k, v in self.variables.items():
            workspace.variables[k] = v.to(device)
        return workspace

    def _convert_to_shared_workspace(self, n_repeat=1, time_size=None):
        &#34;&#34;&#34;INTERNAL METHOD.
        It converts a workspace to a shared workspace, by repeating this workspace n times on the batch dimension
        It also automatically adapts the time_size if specified (used in NRemoteAgent.create)
        &#34;&#34;&#34;

        with torch.no_grad():
            workspace = Workspace()
            for k, v in self.variables.items():
                value = v.get_full(None).detach()
                if time_size is not None:
                    s = value.size()
                    value = torch.zeros(
                        time_size, *s[1:], dtype=value.dtype, device=value.device
                    )
                ts = [value for _ in range(n_repeat)]
                value = torch.cat(ts, dim=1)
                workspace.variables[k] = CompactSharedTensor(value)
                workspace.is_shared = True
        return workspace

    def subtime(self, from_t: int, to_t: int) -&gt; Workspace:
        &#34;&#34;&#34;
        Return a workspace restricted to a subset of the time dimension
        &#34;&#34;&#34;
        assert (
            self._all_variables_same_time_size()
        ), &#34;All variables must have the same time size&#34;
        workspace = Workspace()
        for k, v in self.variables.items():
            workspace.variables[k] = v.subtime(from_t, to_t)
        return workspace

    def remove_variable(self, var_name: str):
        &#34;&#34;&#34;Remove a variable from the Workspace&#34;&#34;&#34;
        del self.variables[var_name]

    def __str__(self):
        r = [&#34;Workspace:&#34;]
        for k, v in self.variables.items():
            r.append(
                &#34;\t&#34;
                + k
                + &#34;: time_size = &#34;
                + str(v.time_size())
                + &#34;, batch_size = &#34;
                + str(v.batch_size())
            )
        return &#34;\n&#34;.join(r)

    def select_subtime(self, t: torch.LongTensor, window_size: int) -&gt; Workspace:
        &#34;&#34;&#34;
        `t` is a tensor of size `batch_size` that provides one time index for each element of the workspace.
        Then the function returns a new workspace by aggregating `window_size` timesteps starting from index `t`
        This methods allows to sample multiple windows in the Workspace.
        Note that the function may be quite slow.
        &#34;&#34;&#34;
        _vars = {k: v.get_full(batch_dims=None) for k, v in self.variables.items()}
        workspace = Workspace()
        for k, v in _vars.items():
            workspace.set_full(
                k, take_per_row_strided(v, t, num_elem=window_size), batch_dims=None
            )
        return workspace

    # Static
    def sample_subworkspace(self, n_times, n_batch_elements, n_timesteps):
        &#34;&#34;&#34;Sample a workspace from the  workspace. The process is the following:
                * Let us consider that workspace batch_size is B and time_size is T
                * For n_times iterations:
                    * We sample a time window of size n_timesteps
                    * We then sample a n_batch_elements elements on the batch size
                    * =&gt;&gt; we obtain a worspace of size n_batch_elements x n_timesteps
                * We concatenate all the workspaces collected (over the batch dimension)

        Args:
            n_times ([type]): The number of sub workspaces to sample (and concatenate)
            n_batch_elements ([type]): &lt;=workspace.batch_size(): nb of batch elements to sample for each subworkspace
            n_timesteps ([type]): &lt;=workspace.time_size(): the number of timesteps to keep

        Returns:
            [Workspace]: The resulting workspace
        &#34;&#34;&#34;
        b = self.batch_size()
        t = self.time_size()
        to_aggregate = []
        for _ in range(n_times):
            assert not n_timesteps &gt; t
            mini_workspace = self
            if n_timesteps &lt; t:
                t = np.random.randint(t - n_timesteps)
                mini_workspace = self.subtime(t, t + n_timesteps)

            # Batch sampling
            if n_batch_elements &lt; b:
                idx_envs = torch.randperm(b)[:n_batch_elements]
                mini_workspace = mini_workspace.select_batch(idx_envs)
            to_aggregate.append(mini_workspace)

        if len(to_aggregate) &gt; 1:
            mini_workspace = Workspace.cat_batch(to_aggregate)
        else:
            mini_workspace = to_aggregate[0]
        return mini_workspace

    def get_transitions(self) -&gt; Workspace:
        &#34;&#34;&#34;
        Takes in a workspace from salina:
        [(step1),(step2),(step3), ... ]
        return a workspace of transitions :
        [
            [step1,step2],
            [step2,step3]
            ...
        ]
        Filters every transitions [step_final,step_initial]
        &#34;&#34;&#34;
        transitions = {}
        done = self[&#34;env/done&#34;][:-1]
        for key in self.keys():
            array = self[key]

            # remove transitions (s_terminal -&gt; s_initial)
            x = array[:-1][~done]
            x_next = array[1:][~done]
            transitions[key] = torch.stack([x, x_next])

        workspace = Workspace()
        for k, v in transitions.items():
            workspace.set_full(k, v)
        return workspace

    def debug_transitions(self, truncated):
        &#34;&#34;&#34; &#34;&#34;&#34;
        critic, done, action_probs, reward, action = self[
            &#34;critic&#34;, &#34;env/done&#34;, &#34;action_probs&#34;, &#34;env/reward&#34;, &#34;action&#34;
        ]
        timestep = self[&#34;env/timestep&#34;]
        assert not done[
            0
        ].max()  # dones is must be always false in the first timestep of the transition.
        # if not it means we have a transition (step final) =&gt; (step initial)

        # timesteps must always follow each other.
        assert (timestep[0] == timestep[1] - 1).all()

        assert (
            truncated[not done].sum().item() == 0
        )  # when done is false, truncated is always false

        if done[truncated].numel() &gt; 0:
            assert torch.amin(
                done[truncated]
            )  # when truncated is true, done is always true
        assert reward[1].sum() == len(
            reward[1]
        ), &#34;in cartpole, rewards are always 1&#34;  # only 1 rewards</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="bbrl.workspace.Workspace.batch_size"><code class="name flex">
<span>def <span class="ident">batch_size</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Return the batch size of the variables in the workspace</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch_size(self) -&gt; int:
    &#34;&#34;&#34;Return the batch size of the variables in the workspace&#34;&#34;&#34;
    _bs = None
    for k, v in self.variables.items():
        if _bs is None:
            _bs = v.batch_size()
        assert _bs == v.batch_size(), &#34;Variables must have the same batch size&#34;
    return _bs</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.cat_batch"><code class="name flex">
<span>def <span class="ident">cat_batch</span></span>(<span>self, workspaces: list[<a title="bbrl.workspace.Workspace" href="#bbrl.workspace.Workspace">Workspace</a>]) ‑> <a title="bbrl.workspace.Workspace" href="#bbrl.workspace.Workspace">Workspace</a></span>
</code></dt>
<dd>
<div class="desc"><p>Concatenate multiple workspaces over the batch dimension.
The workspaces must have the same time dimension.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cat_batch(self, workspaces: list[Workspace]) -&gt; Workspace:
    &#34;&#34;&#34;Concatenate multiple workspaces over the batch dimension.
    The workspaces must have the same time dimension.
    &#34;&#34;&#34;

    ts = None
    for w in workspaces:
        if ts is None:
            ts = w.time_size()
        assert ts == w.time_size(), &#34;Workspaces must have the same time size&#34;

    workspace = Workspace()
    for k in workspaces[0].keys():
        vals = [w[k] for w in workspaces]
        v = torch.cat(vals, dim=1)
        workspace.set_full(k, v)
    return workspace</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.clear"><code class="name flex">
<span>def <span class="ident">clear</span></span>(<span>self, name=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Remove all the variables from the workspace</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clear(self, name=None):
    &#34;&#34;&#34;Remove all the variables from the workspace&#34;&#34;&#34;
    if name is None:
        for k, v in self.variables.items():
            v.clear()
        self.variables = {}
    else:
        self.variables[name].clear()
        del self.variables[name]</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.contiguous"><code class="name flex">
<span>def <span class="ident">contiguous</span></span>(<span>self) ‑> <a title="bbrl.workspace.Workspace" href="#bbrl.workspace.Workspace">Workspace</a></span>
</code></dt>
<dd>
<div class="desc"><p>Generates a workspace where all tensors are stored in the Compact format.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def contiguous(self) -&gt; Workspace:
    &#34;&#34;&#34;Generates a workspace where all tensors are stored in the Compact format.&#34;&#34;&#34;
    workspace = Workspace()
    for k in self.keys():
        workspace.set_full(k, self.get_full(k))
    return workspace</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.copy_n_last_steps"><code class="name flex">
<span>def <span class="ident">copy_n_last_steps</span></span>(<span>self, n: int, var_names: Optional[list[str]] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Copy the n last timesteps of each variable to the n first timesteps.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy_n_last_steps(self, n: int, var_names: Optional[list[str]] = None) -&gt; None:
    &#34;&#34;&#34;Copy the n last timesteps of each variable to the n first timesteps.&#34;&#34;&#34;
    _ts = None
    for k, v in self.variables.items():
        if var_names is None or k in var_names:
            if _ts is None:
                _ts = v.time_size()
            assert _ts == v.time_size(), (
                &#34;Variables must have the same time size: &#34;
                + str(_ts)
                + &#34; vs &#34;
                + str(v.time_size())
            )

    for k, v in self.variables.items():
        if var_names is None or k in var_names:
            self.copy_time(_ts - n, 0, n)</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.copy_time"><code class="name flex">
<span>def <span class="ident">copy_time</span></span>(<span>self, from_time: int, to_time: int, n_steps: int, var_names: Optional[list[str]] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Copy all the variables values from time <code>from_time</code> to <code>from_time+n_steps</code>
to <code>to_time</code> to <code>to_time+n_steps</code>
It can be restricted to specific variables using <code>var_names</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy_time(
    self,
    from_time: int,
    to_time: int,
    n_steps: int,
    var_names: Optional[list[str]] = None,
):
    &#34;&#34;&#34;Copy all the variables values from time `from_time` to `from_time+n_steps`
    to `to_time` to `to_time+n_steps`
    It can be restricted to specific variables using `var_names`.
    &#34;&#34;&#34;
    for k, v in self.variables.items():
        if var_names is None or k in var_names:
            v.copy_time(from_time, to_time, n_steps)</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.debug_transitions"><code class="name flex">
<span>def <span class="ident">debug_transitions</span></span>(<span>self, truncated)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def debug_transitions(self, truncated):
    &#34;&#34;&#34; &#34;&#34;&#34;
    critic, done, action_probs, reward, action = self[
        &#34;critic&#34;, &#34;env/done&#34;, &#34;action_probs&#34;, &#34;env/reward&#34;, &#34;action&#34;
    ]
    timestep = self[&#34;env/timestep&#34;]
    assert not done[
        0
    ].max()  # dones is must be always false in the first timestep of the transition.
    # if not it means we have a transition (step final) =&gt; (step initial)

    # timesteps must always follow each other.
    assert (timestep[0] == timestep[1] - 1).all()

    assert (
        truncated[not done].sum().item() == 0
    )  # when done is false, truncated is always false

    if done[truncated].numel() &gt; 0:
        assert torch.amin(
            done[truncated]
        )  # when truncated is true, done is always true
    assert reward[1].sum() == len(
        reward[1]
    ), &#34;in cartpole, rewards are always 1&#34;  # only 1 rewards</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self, var_name: str, t: int, batch_dims: Optional[tuple[int, int]] = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Get the variable var_name at time t</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(
    self, var_name: str, t: int, batch_dims: Optional[tuple[int, int]] = None
) -&gt; torch.Tensor:
    &#34;&#34;&#34;Get the variable var_name at time t&#34;&#34;&#34;
    assert var_name in self.variables, &#34;Unknown variable &#39;&#34; + var_name + &#34;&#39;&#34;
    return self.variables[var_name].get(t, batch_dims=batch_dims)</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.get_full"><code class="name flex">
<span>def <span class="ident">get_full</span></span>(<span>self, var_name: str, batch_dims: Optional[tuple[int, int]] = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Return the complete tensor for var_name</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_full(
    self, var_name: str, batch_dims: Optional[tuple[int, int]] = None
) -&gt; torch.Tensor:
    &#34;&#34;&#34;Return the complete tensor for var_name&#34;&#34;&#34;
    assert var_name in self.variables, (
        &#34;[Workspace.get_full] unknown variable &#39;&#34; + var_name + &#34;&#39;&#34;
    )
    return self.variables[var_name].get_full(batch_dims=batch_dims)</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.get_time_truncated"><code class="name flex">
<span>def <span class="ident">get_time_truncated</span></span>(<span>self, var_name: str, from_time: int, to_time: int, batch_dims: Optional[tuple[int, int]] = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Return workspace[var_name][from_time:to_time]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_time_truncated(
    self,
    var_name: str,
    from_time: int,
    to_time: int,
    batch_dims: Optional[tuple[int, int]] = None,
) -&gt; torch.Tensor:
    &#34;&#34;&#34;Return workspace[var_name][from_time:to_time]&#34;&#34;&#34;
    assert from_time &gt;= 0 and to_time &gt;= 0 and to_time &gt; from_time

    v = self.variables[var_name]
    if isinstance(v, SlicedTemporalTensor):
        return v.get_time_truncated(from_time, to_time, batch_dims)
    else:
        return v.get_full(batch_dims)[from_time:to_time]</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.get_time_truncated_workspace"><code class="name flex">
<span>def <span class="ident">get_time_truncated_workspace</span></span>(<span>self, from_time: int, to_time: int) ‑> <a title="bbrl.workspace.Workspace" href="#bbrl.workspace.Workspace">Workspace</a></span>
</code></dt>
<dd>
<div class="desc"><p>Return a workspace where all variables are truncated between from_time and to_time</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_time_truncated_workspace(self, from_time: int, to_time: int) -&gt; Workspace:
    &#34;&#34;&#34;Return a workspace where all variables are truncated between from_time and to_time&#34;&#34;&#34;
    workspace = Workspace()
    for k in self.keys():
        workspace.set_full(k, self.get_time_truncated(k, from_time, to_time, None))
    return workspace</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.get_transitions"><code class="name flex">
<span>def <span class="ident">get_transitions</span></span>(<span>self) ‑> <a title="bbrl.workspace.Workspace" href="#bbrl.workspace.Workspace">Workspace</a></span>
</code></dt>
<dd>
<div class="desc"><p>Takes in a workspace from salina:
[(step1),(step2),(step3), &hellip; ]
return a workspace of transitions :
[
[step1,step2],
[step2,step3]
&hellip;
]
Filters every transitions [step_final,step_initial]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_transitions(self) -&gt; Workspace:
    &#34;&#34;&#34;
    Takes in a workspace from salina:
    [(step1),(step2),(step3), ... ]
    return a workspace of transitions :
    [
        [step1,step2],
        [step2,step3]
        ...
    ]
    Filters every transitions [step_final,step_initial]
    &#34;&#34;&#34;
    transitions = {}
    done = self[&#34;env/done&#34;][:-1]
    for key in self.keys():
        array = self[key]

        # remove transitions (s_terminal -&gt; s_initial)
        x = array[:-1][~done]
        x_next = array[1:][~done]
        transitions[key] = torch.stack([x, x_next])

    workspace = Workspace()
    for k, v in transitions.items():
        workspace.set_full(k, v)
    return workspace</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.keys"><code class="name flex">
<span>def <span class="ident">keys</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return an iterator over the variables names</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def keys(self):
    &#34;&#34;&#34;Return an iterator over the variables names&#34;&#34;&#34;
    return self.variables.keys()</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.remove_variable"><code class="name flex">
<span>def <span class="ident">remove_variable</span></span>(<span>self, var_name: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Remove a variable from the Workspace</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_variable(self, var_name: str):
    &#34;&#34;&#34;Remove a variable from the Workspace&#34;&#34;&#34;
    del self.variables[var_name]</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.sample_subworkspace"><code class="name flex">
<span>def <span class="ident">sample_subworkspace</span></span>(<span>self, n_times, n_batch_elements, n_timesteps)</span>
</code></dt>
<dd>
<div class="desc"><p>Sample a workspace from the
workspace. The process is the following:
* Let us consider that workspace batch_size is B and time_size is T
* For n_times iterations:
* We sample a time window of size n_timesteps
* We then sample a n_batch_elements elements on the batch size
* =&gt;&gt; we obtain a worspace of size n_batch_elements x n_timesteps
* We concatenate all the workspaces collected (over the batch dimension)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_times</code></strong> :&ensp;<code>[type]</code></dt>
<dd>The number of sub workspaces to sample (and concatenate)</dd>
<dt><strong><code>n_batch_elements</code></strong> :&ensp;<code>[type]</code></dt>
<dd>&lt;=workspace.batch_size(): nb of batch elements to sample for each subworkspace</dd>
<dt><strong><code>n_timesteps</code></strong> :&ensp;<code>[type]</code></dt>
<dd>&lt;=workspace.time_size(): the number of timesteps to keep</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>[<a title="bbrl.workspace.Workspace" href="#bbrl.workspace.Workspace">Workspace</a>]</code></dt>
<dd>The resulting workspace</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_subworkspace(self, n_times, n_batch_elements, n_timesteps):
    &#34;&#34;&#34;Sample a workspace from the  workspace. The process is the following:
            * Let us consider that workspace batch_size is B and time_size is T
            * For n_times iterations:
                * We sample a time window of size n_timesteps
                * We then sample a n_batch_elements elements on the batch size
                * =&gt;&gt; we obtain a worspace of size n_batch_elements x n_timesteps
            * We concatenate all the workspaces collected (over the batch dimension)

    Args:
        n_times ([type]): The number of sub workspaces to sample (and concatenate)
        n_batch_elements ([type]): &lt;=workspace.batch_size(): nb of batch elements to sample for each subworkspace
        n_timesteps ([type]): &lt;=workspace.time_size(): the number of timesteps to keep

    Returns:
        [Workspace]: The resulting workspace
    &#34;&#34;&#34;
    b = self.batch_size()
    t = self.time_size()
    to_aggregate = []
    for _ in range(n_times):
        assert not n_timesteps &gt; t
        mini_workspace = self
        if n_timesteps &lt; t:
            t = np.random.randint(t - n_timesteps)
            mini_workspace = self.subtime(t, t + n_timesteps)

        # Batch sampling
        if n_batch_elements &lt; b:
            idx_envs = torch.randperm(b)[:n_batch_elements]
            mini_workspace = mini_workspace.select_batch(idx_envs)
        to_aggregate.append(mini_workspace)

    if len(to_aggregate) &gt; 1:
        mini_workspace = Workspace.cat_batch(to_aggregate)
    else:
        mini_workspace = to_aggregate[0]
    return mini_workspace</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.select_batch"><code class="name flex">
<span>def <span class="ident">select_batch</span></span>(<span>self, batch_indexes: torch.LongTensor) ‑> <a title="bbrl.workspace.Workspace" href="#bbrl.workspace.Workspace">Workspace</a></span>
</code></dt>
<dd>
<div class="desc"><p>Given a tensor of indexes, returns a new workspace
with the selected elements (over the batch dimension)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_batch(self, batch_indexes: torch.LongTensor) -&gt; Workspace:
    &#34;&#34;&#34;Given a tensor of indexes, returns a new workspace
    with the selected elements (over the batch dimension)
    &#34;&#34;&#34;
    _bs = None
    for k, v in self.variables.items():
        if _bs is None:
            _bs = v.batch_size()
        assert _bs == v.batch_size(), &#34;Variables must have the same batch size&#34;

    workspace = Workspace()
    for k, v in self.variables.items():
        v = v.select_batch(batch_indexes)
        workspace.variables[k] = v
    return workspace</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.select_batch_n"><code class="name flex">
<span>def <span class="ident">select_batch_n</span></span>(<span>self, n)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a new Workspace of batch_size==n by randomly sampling over the batch dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_batch_n(self, n):
    &#34;&#34;&#34;Return a new Workspace of batch_size==n by randomly sampling over the batch dimensions&#34;&#34;&#34;
    who = torch.randint(low=0, high=self.batch_size(), size=(n,))
    return self.select_batch(who)</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.select_subtime"><code class="name flex">
<span>def <span class="ident">select_subtime</span></span>(<span>self, t: torch.LongTensor, window_size: int) ‑> <a title="bbrl.workspace.Workspace" href="#bbrl.workspace.Workspace">Workspace</a></span>
</code></dt>
<dd>
<div class="desc"><p><code>t</code> is a tensor of size <code>batch_size</code> that provides one time index for each element of the workspace.
Then the function returns a new workspace by aggregating <code>window_size</code> timesteps starting from index <code>t</code>
This methods allows to sample multiple windows in the Workspace.
Note that the function may be quite slow.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_subtime(self, t: torch.LongTensor, window_size: int) -&gt; Workspace:
    &#34;&#34;&#34;
    `t` is a tensor of size `batch_size` that provides one time index for each element of the workspace.
    Then the function returns a new workspace by aggregating `window_size` timesteps starting from index `t`
    This methods allows to sample multiple windows in the Workspace.
    Note that the function may be quite slow.
    &#34;&#34;&#34;
    _vars = {k: v.get_full(batch_dims=None) for k, v in self.variables.items()}
    workspace = Workspace()
    for k, v in _vars.items():
        workspace.set_full(
            k, take_per_row_strided(v, t, num_elem=window_size), batch_dims=None
        )
    return workspace</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.set"><code class="name flex">
<span>def <span class="ident">set</span></span>(<span>self, var_name: str, t: int, v: torch.Tensor, batch_dims: Optional[tuple[int, int]] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the variable var_name at time t</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set(
    self,
    var_name: str,
    t: int,
    v: torch.Tensor,
    batch_dims: Optional[tuple[int, int]] = None,
):
    &#34;&#34;&#34;Set the variable var_name at time t&#34;&#34;&#34;
    if var_name not in self.variables:
        assert not self.is_shared, &#34;Cannot add new variable into a shared workspace&#34;
        self.variables[var_name] = SlicedTemporalTensor()
    elif isinstance(self.variables[var_name], CompactTemporalTensor):
        self.variables[var_name] = self.variables[var_name].to_sliced()

    self.variables[var_name].set(t, v, batch_dims=batch_dims)</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.set_full"><code class="name flex">
<span>def <span class="ident">set_full</span></span>(<span>self, var_name: str, value: torch.Tensor, batch_dims: Optional[tuple[int, int]] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Set variable var_name with a complete tensor (TxBx&hellip;) where T is the time dimension
and B is the batch size</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_full(
    self,
    var_name: str,
    value: torch.Tensor,
    batch_dims: Optional[tuple[int, int]] = None,
):
    &#34;&#34;&#34;Set variable var_name with a complete tensor (TxBx...) where T is the time dimension
    and B is the batch size
    &#34;&#34;&#34;
    if var_name not in self.variables:
        assert not self.is_shared, &#34;Cannot add new variable into a shared workspace&#34;
        self.variables[var_name] = CompactTemporalTensor()
    self.variables[var_name].set_full(value, batch_dims=batch_dims)</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.subtime"><code class="name flex">
<span>def <span class="ident">subtime</span></span>(<span>self, from_t: int, to_t: int) ‑> <a title="bbrl.workspace.Workspace" href="#bbrl.workspace.Workspace">Workspace</a></span>
</code></dt>
<dd>
<div class="desc"><p>Return a workspace restricted to a subset of the time dimension</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def subtime(self, from_t: int, to_t: int) -&gt; Workspace:
    &#34;&#34;&#34;
    Return a workspace restricted to a subset of the time dimension
    &#34;&#34;&#34;
    assert (
        self._all_variables_same_time_size()
    ), &#34;All variables must have the same time size&#34;
    workspace = Workspace()
    for k, v in self.variables.items():
        workspace.variables[k] = v.subtime(from_t, to_t)
    return workspace</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.time_size"><code class="name flex">
<span>def <span class="ident">time_size</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Return the time size of the variables in the workspace</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def time_size(self) -&gt; int:
    &#34;&#34;&#34;Return the time size of the variables in the workspace&#34;&#34;&#34;
    _ts = None
    for k, v in self.variables.items():
        if _ts is None:
            _ts = v.time_size()
        assert _ts == v.time_size(), &#34;Variables must have the same time size&#34;
    return _ts</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.to"><code class="name flex">
<span>def <span class="ident">to</span></span>(<span>self, device: torch.device) ‑> <a title="bbrl.workspace.Workspace" href="#bbrl.workspace.Workspace">Workspace</a></span>
</code></dt>
<dd>
<div class="desc"><p>Return a workspace where all tensors are on a particular device</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to(self, device: torch.device) -&gt; Workspace:
    &#34;&#34;&#34;Return a workspace where all tensors are on a particular device&#34;&#34;&#34;
    workspace = Workspace()
    for k, v in self.variables.items():
        workspace.variables[k] = v.to(device)
    return workspace</code></pre>
</details>
</dd>
<dt id="bbrl.workspace.Workspace.zero_grad"><code class="name flex">
<span>def <span class="ident">zero_grad</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Remove any gradient information</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zero_grad(self) -&gt; None:
    &#34;&#34;&#34;Remove any gradient information&#34;&#34;&#34;
    for k, v in self.variables.items():
        v.zero_grad()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="bbrl" href="index.html">bbrl</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="bbrl.workspace.take_per_row_strided" href="#bbrl.workspace.take_per_row_strided">take_per_row_strided</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="bbrl.workspace.CompactSharedTensor" href="#bbrl.workspace.CompactSharedTensor">CompactSharedTensor</a></code></h4>
<ul class="two-column">
<li><code><a title="bbrl.workspace.CompactSharedTensor.batch_size" href="#bbrl.workspace.CompactSharedTensor.batch_size">batch_size</a></code></li>
<li><code><a title="bbrl.workspace.CompactSharedTensor.clear" href="#bbrl.workspace.CompactSharedTensor.clear">clear</a></code></li>
<li><code><a title="bbrl.workspace.CompactSharedTensor.copy_time" href="#bbrl.workspace.CompactSharedTensor.copy_time">copy_time</a></code></li>
<li><code><a title="bbrl.workspace.CompactSharedTensor.get" href="#bbrl.workspace.CompactSharedTensor.get">get</a></code></li>
<li><code><a title="bbrl.workspace.CompactSharedTensor.get_full" href="#bbrl.workspace.CompactSharedTensor.get_full">get_full</a></code></li>
<li><code><a title="bbrl.workspace.CompactSharedTensor.select_batch" href="#bbrl.workspace.CompactSharedTensor.select_batch">select_batch</a></code></li>
<li><code><a title="bbrl.workspace.CompactSharedTensor.set" href="#bbrl.workspace.CompactSharedTensor.set">set</a></code></li>
<li><code><a title="bbrl.workspace.CompactSharedTensor.set_full" href="#bbrl.workspace.CompactSharedTensor.set_full">set_full</a></code></li>
<li><code><a title="bbrl.workspace.CompactSharedTensor.subtime" href="#bbrl.workspace.CompactSharedTensor.subtime">subtime</a></code></li>
<li><code><a title="bbrl.workspace.CompactSharedTensor.time_size" href="#bbrl.workspace.CompactSharedTensor.time_size">time_size</a></code></li>
<li><code><a title="bbrl.workspace.CompactSharedTensor.to" href="#bbrl.workspace.CompactSharedTensor.to">to</a></code></li>
<li><code><a title="bbrl.workspace.CompactSharedTensor.zero_grad" href="#bbrl.workspace.CompactSharedTensor.zero_grad">zero_grad</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bbrl.workspace.CompactTemporalTensor" href="#bbrl.workspace.CompactTemporalTensor">CompactTemporalTensor</a></code></h4>
<ul class="two-column">
<li><code><a title="bbrl.workspace.CompactTemporalTensor.batch_size" href="#bbrl.workspace.CompactTemporalTensor.batch_size">batch_size</a></code></li>
<li><code><a title="bbrl.workspace.CompactTemporalTensor.clear" href="#bbrl.workspace.CompactTemporalTensor.clear">clear</a></code></li>
<li><code><a title="bbrl.workspace.CompactTemporalTensor.copy_time" href="#bbrl.workspace.CompactTemporalTensor.copy_time">copy_time</a></code></li>
<li><code><a title="bbrl.workspace.CompactTemporalTensor.get" href="#bbrl.workspace.CompactTemporalTensor.get">get</a></code></li>
<li><code><a title="bbrl.workspace.CompactTemporalTensor.get_full" href="#bbrl.workspace.CompactTemporalTensor.get_full">get_full</a></code></li>
<li><code><a title="bbrl.workspace.CompactTemporalTensor.select_batch" href="#bbrl.workspace.CompactTemporalTensor.select_batch">select_batch</a></code></li>
<li><code><a title="bbrl.workspace.CompactTemporalTensor.set" href="#bbrl.workspace.CompactTemporalTensor.set">set</a></code></li>
<li><code><a title="bbrl.workspace.CompactTemporalTensor.set_full" href="#bbrl.workspace.CompactTemporalTensor.set_full">set_full</a></code></li>
<li><code><a title="bbrl.workspace.CompactTemporalTensor.subtime" href="#bbrl.workspace.CompactTemporalTensor.subtime">subtime</a></code></li>
<li><code><a title="bbrl.workspace.CompactTemporalTensor.time_size" href="#bbrl.workspace.CompactTemporalTensor.time_size">time_size</a></code></li>
<li><code><a title="bbrl.workspace.CompactTemporalTensor.to" href="#bbrl.workspace.CompactTemporalTensor.to">to</a></code></li>
<li><code><a title="bbrl.workspace.CompactTemporalTensor.to_sliced" href="#bbrl.workspace.CompactTemporalTensor.to_sliced">to_sliced</a></code></li>
<li><code><a title="bbrl.workspace.CompactTemporalTensor.zero_grad" href="#bbrl.workspace.CompactTemporalTensor.zero_grad">zero_grad</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bbrl.workspace.SlicedTemporalTensor" href="#bbrl.workspace.SlicedTemporalTensor">SlicedTemporalTensor</a></code></h4>
<ul class="two-column">
<li><code><a title="bbrl.workspace.SlicedTemporalTensor.batch_size" href="#bbrl.workspace.SlicedTemporalTensor.batch_size">batch_size</a></code></li>
<li><code><a title="bbrl.workspace.SlicedTemporalTensor.clear" href="#bbrl.workspace.SlicedTemporalTensor.clear">clear</a></code></li>
<li><code><a title="bbrl.workspace.SlicedTemporalTensor.copy_time" href="#bbrl.workspace.SlicedTemporalTensor.copy_time">copy_time</a></code></li>
<li><code><a title="bbrl.workspace.SlicedTemporalTensor.get" href="#bbrl.workspace.SlicedTemporalTensor.get">get</a></code></li>
<li><code><a title="bbrl.workspace.SlicedTemporalTensor.get_full" href="#bbrl.workspace.SlicedTemporalTensor.get_full">get_full</a></code></li>
<li><code><a title="bbrl.workspace.SlicedTemporalTensor.get_time_truncated" href="#bbrl.workspace.SlicedTemporalTensor.get_time_truncated">get_time_truncated</a></code></li>
<li><code><a title="bbrl.workspace.SlicedTemporalTensor.select_batch" href="#bbrl.workspace.SlicedTemporalTensor.select_batch">select_batch</a></code></li>
<li><code><a title="bbrl.workspace.SlicedTemporalTensor.set" href="#bbrl.workspace.SlicedTemporalTensor.set">set</a></code></li>
<li><code><a title="bbrl.workspace.SlicedTemporalTensor.set_full" href="#bbrl.workspace.SlicedTemporalTensor.set_full">set_full</a></code></li>
<li><code><a title="bbrl.workspace.SlicedTemporalTensor.subtime" href="#bbrl.workspace.SlicedTemporalTensor.subtime">subtime</a></code></li>
<li><code><a title="bbrl.workspace.SlicedTemporalTensor.time_size" href="#bbrl.workspace.SlicedTemporalTensor.time_size">time_size</a></code></li>
<li><code><a title="bbrl.workspace.SlicedTemporalTensor.to" href="#bbrl.workspace.SlicedTemporalTensor.to">to</a></code></li>
<li><code><a title="bbrl.workspace.SlicedTemporalTensor.zero_grad" href="#bbrl.workspace.SlicedTemporalTensor.zero_grad">zero_grad</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bbrl.workspace.Workspace" href="#bbrl.workspace.Workspace">Workspace</a></code></h4>
<ul class="">
<li><code><a title="bbrl.workspace.Workspace.batch_size" href="#bbrl.workspace.Workspace.batch_size">batch_size</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.cat_batch" href="#bbrl.workspace.Workspace.cat_batch">cat_batch</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.clear" href="#bbrl.workspace.Workspace.clear">clear</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.contiguous" href="#bbrl.workspace.Workspace.contiguous">contiguous</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.copy_n_last_steps" href="#bbrl.workspace.Workspace.copy_n_last_steps">copy_n_last_steps</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.copy_time" href="#bbrl.workspace.Workspace.copy_time">copy_time</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.debug_transitions" href="#bbrl.workspace.Workspace.debug_transitions">debug_transitions</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.get" href="#bbrl.workspace.Workspace.get">get</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.get_full" href="#bbrl.workspace.Workspace.get_full">get_full</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.get_time_truncated" href="#bbrl.workspace.Workspace.get_time_truncated">get_time_truncated</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.get_time_truncated_workspace" href="#bbrl.workspace.Workspace.get_time_truncated_workspace">get_time_truncated_workspace</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.get_transitions" href="#bbrl.workspace.Workspace.get_transitions">get_transitions</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.keys" href="#bbrl.workspace.Workspace.keys">keys</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.remove_variable" href="#bbrl.workspace.Workspace.remove_variable">remove_variable</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.sample_subworkspace" href="#bbrl.workspace.Workspace.sample_subworkspace">sample_subworkspace</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.select_batch" href="#bbrl.workspace.Workspace.select_batch">select_batch</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.select_batch_n" href="#bbrl.workspace.Workspace.select_batch_n">select_batch_n</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.select_subtime" href="#bbrl.workspace.Workspace.select_subtime">select_subtime</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.set" href="#bbrl.workspace.Workspace.set">set</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.set_full" href="#bbrl.workspace.Workspace.set_full">set_full</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.subtime" href="#bbrl.workspace.Workspace.subtime">subtime</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.time_size" href="#bbrl.workspace.Workspace.time_size">time_size</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.to" href="#bbrl.workspace.Workspace.to">to</a></code></li>
<li><code><a title="bbrl.workspace.Workspace.zero_grad" href="#bbrl.workspace.Workspace.zero_grad">zero_grad</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>