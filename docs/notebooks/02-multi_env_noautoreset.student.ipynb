{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b34dc4e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Outlook\n",
    "\n",
    "This notebook is designed to understand how to use a gymnasium environment as a BBRL agent in practice, using autoreset=False.\n",
    "It is part of the [BBRL documentation](https://github.com/osigaud/bbrl/docs/index.html).\n",
    "\n",
    "If this is your first contact with BBRL, you may start be having a look at [this more basic notebook](01-basic_concepts.student.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce2d0a6",
   "metadata": {},
   "source": [
    "## Installation and Imports\n",
    "\n",
    "The BBRL library is [here](https://github.com/osigaud/bbrl).\n",
    "\n",
    "Below, we import standard python packages, pytorch packages and gymnasium environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8daeda",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[easypip] Installing bbrl_gymnasium>=0.2.0\n",
      "[easypip] Installing bbrl_gymnasium[classic_control]\n"
     ]
    }
   ],
   "source": [
    "# Installs the necessary Python and system libraries\n",
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError as e:\n",
    "    get_ipython().run_line_magic(\"pip\", \"install easypip\")\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
    "easyinstall(\"bbrl_gymnasium[classic_control]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3c0ede2",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "from moviepy.editor import ipython_display as video_display\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Optional\n",
    "from functools import partial\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import bbrl_gymnasium\n",
    "\n",
    "import copy\n",
    "from abc import abstractmethod, ABC\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import strftime\n",
    "OmegaConf.register_new_resolver(\n",
    "    \"current_time\", lambda: strftime(\"%Y%m%d-%H%M%S\"), replace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "646bd8de",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Imports all the necessary classes and functions from BBRL\n",
    "from bbrl.agents.agent import Agent\n",
    "from bbrl import get_arguments, get_class, instantiate_class\n",
    "# The workspace is the main class in BBRL, this is where all data is collected and stored\n",
    "from bbrl.workspace import Workspace\n",
    "\n",
    "# Agents(agent1, agent2, agent3, ...) executes the different agents the one after the other\n",
    "# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n",
    "# or until a given condition is reached\n",
    "\n",
    "from bbrl.agents import Agents, TemporalAgent\n",
    "from bbrl.agents.gymnasium import ParallelGymAgent, make_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1949cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers.time_limit import TimeLimit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e38c6d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Definition of agents\n",
    "\n",
    "We first create an Agent representing [the CartPole-v1 gym environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/).\n",
    "This is done using the [ParallelGymAgent](https://github.com/osigaud/bbrl/blob/40fe0468feb8998e62c3cd6bb3a575fef88e256f/src/bbrl/agents/gymnasium.py#L261) class.\n",
    "\n",
    "The ParallelGymAgent is an agent able to execute a batch of gymnasium environments\n",
    "with or without auto-resetting. These agents produce multiple variables in the workspace:\n",
    "’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/terminated’,\n",
    "'env/truncated', 'env/done', ’env/cumulated_reward’.\n",
    "\n",
    "When called at timestep t=0, the environments are automatically reset. At\n",
    "timestep t>0, these agents will read the ’action’ variable in the workspace at\n",
    "time t − 1 to generate the next state, by calling the step(action) of the contained gymnasium environment.\n",
    "\n",
    "In the example below, we are working with batches (i.e. several episodes at the same time),\n",
    "so here our agent uses `n_envs = 3` environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce253c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: observation space in R^4 and action space {1, ..., 2}\n"
     ]
    }
   ],
   "source": [
    "# We run episodes over 3 environments at a time\n",
    "n_envs = 3\n",
    "env_agent = ParallelGymAgent(partial(make_env, 'CartPole-v1', autoreset=False, wrappers=[lambda x: TimeLimit(x,5)]), n_envs, reward_at_t=False)\n",
    "# The random seed is set to 2139\n",
    "env_agent.seed(2139)\n",
    "\n",
    "obs_size, action_dim = env_agent.get_obs_and_actions_sizes()\n",
    "print(f\"Environment: observation space in R^{obs_size} and action space {{1, ..., {action_dim}}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c28027b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation tensor([[-0.0085, -0.0427, -0.0489,  0.0215],\n",
      "        [ 0.0005,  0.0025, -0.0493, -0.0402],\n",
      "        [ 0.0080,  0.0203, -0.0023, -0.0085]])\n"
     ]
    }
   ],
   "source": [
    "# Creates a new workspace\n",
    "workspace = Workspace() \n",
    "\n",
    "# Execute the first step\n",
    "env_agent(workspace, t=0)\n",
    "\n",
    "# Our first set of observations. The size of the observation space is 4, and we have 3 environments.\n",
    "obs = workspace.get(\"env/env_obs\", 0)\n",
    "print(\"Observation\", obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db311a2",
   "metadata": {},
   "source": [
    "To generate more steps into the workspace, we need to send actions to the environment.\n",
    "\n",
    "### Random action without agent\n",
    "\n",
    "We first set an action directly without using an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92877535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0094,  0.1531, -0.0485, -0.2862],\n",
       "        [ 0.0006, -0.1919, -0.0501,  0.2366],\n",
       "        [ 0.0084,  0.2155, -0.0025, -0.3019]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sets the next action\n",
    "action = torch.randint(0, action_dim, (n_envs, ))\n",
    "workspace.set(\"action\", 0, action)\n",
    "print(action)\n",
    "env_agent(workspace, t=1)\n",
    "\n",
    "# And perform one step\n",
    "workspace.get(\"env/env_obs\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efe6d64",
   "metadata": {},
   "source": [
    "Let us now look at what's in the workspace. You can see below all the variables it generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8e1e6e7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env/env_obs tensor([[[-0.0085, -0.0427, -0.0489,  0.0215],\n",
      "         [ 0.0005,  0.0025, -0.0493, -0.0402],\n",
      "         [ 0.0080,  0.0203, -0.0023, -0.0085]],\n",
      "\n",
      "        [[-0.0094,  0.1531, -0.0485, -0.2862],\n",
      "         [ 0.0006, -0.1919, -0.0501,  0.2366],\n",
      "         [ 0.0084,  0.2155, -0.0025, -0.3019]]])\n",
      "env/terminated tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "env/truncated tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "env/done tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "env/reward tensor([[0., 0., 0.],\n",
      "        [1., 1., 1.]])\n",
      "env/cumulated_reward tensor([[0., 0., 0.],\n",
      "        [1., 1., 1.]])\n",
      "env/timestep tensor([[0, 0, 0],\n",
      "        [1, 1, 1]])\n",
      "action tensor([[1, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "for key in workspace.variables.keys():\n",
    "    print(key, workspace[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e810df8",
   "metadata": {},
   "source": [
    "You can observe that we have two time steps for each variable that are stored\n",
    "within tensors where the first dimension is time.\n",
    "\n",
    "You can also see that by convention, all variables written by the environment start with \"env/\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e305632",
   "metadata": {},
   "source": [
    "### Random agent\n",
    "\n",
    "The process above can be\n",
    "automatized with `Agents` and `TemporalAgent` as shown below - but first we have\n",
    "to create an agent that selects the actions (here, randomly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7133853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def forward(self, t: int, choose_action=True, **kwargs):\n",
    "        \"\"\"An Agent can use self.workspace\"\"\"\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = torch.randint(0, self.action_dim, (len(obs), ))\n",
    "        self.set((\"action\", t), action)\n",
    "\n",
    "# Each agent is run in the order given when constructing Agents\n",
    "agents = Agents(env_agent, RandomAgent(action_dim))\n",
    "\n",
    "# And the TemporalAgent allows to run through time\n",
    "t_agents = TemporalAgent(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb791f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now run the agents throught time with a simple call...\n",
    "\n",
    "workspace = Workspace()\n",
    "t_agents(workspace, t=0, stop_variable=\"env/done\", stochastic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "262e3b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env/env_obs tensor([[[ 1.2417e-02, -1.1647e-02,  2.1894e-02,  4.7717e-02],\n",
      "         [ 1.0013e-02, -9.4643e-04, -1.0945e-02, -6.8630e-03],\n",
      "         [ 4.5724e-02,  2.0465e-02,  4.8711e-02,  3.0704e-03]],\n",
      "\n",
      "        [[ 1.2184e-02,  1.8315e-01,  2.2849e-02, -2.3798e-01],\n",
      "         [ 9.9937e-03,  1.9433e-01, -1.1082e-02, -3.0298e-01],\n",
      "         [ 4.6133e-02, -1.7532e-01,  4.8772e-02,  3.1072e-01]],\n",
      "\n",
      "        [[ 1.5847e-02,  3.7794e-01,  1.8089e-02, -5.2337e-01],\n",
      "         [ 1.3880e-02, -6.3152e-04, -1.7141e-02, -1.3811e-02],\n",
      "         [ 4.2627e-02, -3.7110e-01,  5.4987e-02,  6.1837e-01]],\n",
      "\n",
      "        [[ 2.3406e-02,  1.8257e-01,  7.6216e-03, -2.2504e-01],\n",
      "         [ 1.3868e-02, -1.9550e-01, -1.7418e-02,  2.7341e-01],\n",
      "         [ 3.5205e-02, -5.6695e-01,  6.7354e-02,  9.2785e-01]],\n",
      "\n",
      "        [[ 2.7057e-02, -1.2660e-02,  3.1208e-03,  7.0038e-02],\n",
      "         [ 9.9577e-03, -3.9037e-01, -1.1949e-02,  5.6055e-01],\n",
      "         [ 2.3866e-02, -3.7280e-01,  8.5911e-02,  6.5707e-01]],\n",
      "\n",
      "        [[ 2.6804e-02,  1.8242e-01,  4.5216e-03, -2.2166e-01],\n",
      "         [ 2.1502e-03, -1.9509e-01, -7.3831e-04,  2.6413e-01],\n",
      "         [ 1.6410e-02, -1.7897e-01,  9.9053e-02,  3.9263e-01]]])\n",
      "env/terminated tensor([[False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False]])\n",
      "env/truncated tensor([[False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False],\n",
      "        [ True,  True,  True]])\n",
      "env/done tensor([[False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False],\n",
      "        [ True,  True,  True]])\n",
      "env/reward tensor([[0., 0., 0.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "env/cumulated_reward tensor([[0., 0., 0.],\n",
      "        [1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.],\n",
      "        [4., 4., 4.],\n",
      "        [5., 5., 5.]])\n",
      "env/timestep tensor([[0, 0, 0],\n",
      "        [1, 1, 1],\n",
      "        [2, 2, 2],\n",
      "        [3, 3, 3],\n",
      "        [4, 4, 4],\n",
      "        [5, 5, 5]])\n",
      "action tensor([[1, 1, 0],\n",
      "        [1, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 1],\n",
      "        [1, 1, 1],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "for key in workspace.variables.keys():\n",
    "    print(key, workspace[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f2d21e",
   "metadata": {},
   "source": [
    "### Termination\n",
    "\n",
    "`env/done` tells us whether the episode was finished or not (it is either terminated or truncated)\n",
    "here, with NoAutoReset, we wait that all episodes are \"done\"\n",
    "and when the episode is finished, the variables are copied for that environment until all episodes are done.\n",
    "So, when an environment is done before the others, its content is copied until the termination of all environments.\n",
    "This is convenient for collecting the final reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a3bdf1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([61, 3]),\n",
       " tensor([[ True,  True, False],\n",
       "         [ True,  True, False],\n",
       "         [ True,  True, False],\n",
       "         [ True,  True, False],\n",
       "         [ True,  True, False],\n",
       "         [ True,  True, False],\n",
       "         [ True,  True, False],\n",
       "         [ True,  True, False],\n",
       "         [ True,  True, False],\n",
       "         [ True,  True,  True]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace[\"env/done\"].shape, workspace[\"env/done\"][-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a4ff4",
   "metadata": {},
   "source": [
    "You can see that the variable is copied until all episodes are done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800669ec",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "The resulting tensor of observations, with the last two observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66cc8624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([61, 3, 4]),\n",
       " tensor([[[ 0.1846,  1.1643, -0.2254, -1.8629],\n",
       "          [ 0.1352,  0.9839, -0.2152, -1.7081],\n",
       "          [ 0.6511,  2.1055, -0.1729, -1.8871]],\n",
       " \n",
       "         [[ 0.1846,  1.1643, -0.2254, -1.8629],\n",
       "          [ 0.1352,  0.9839, -0.2152, -1.7081],\n",
       "          [ 0.6932,  2.3020, -0.2107, -2.2281]]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace[\"env/env_obs\"].shape, workspace[\"env/env_obs\"][-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2b6c6d",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "\n",
    "The resulting tensor of rewards, with the last 8 rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c046115f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([61, 3]),\n",
       " tensor([[0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace[\"env/reward\"].shape, workspace[\"env/reward\"][-8:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f417da",
   "metadata": {},
   "source": [
    "and the cumulated rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e12520b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([61, 3]),\n",
       " tensor([[14., 17., 53.],\n",
       "         [14., 17., 54.],\n",
       "         [14., 17., 55.],\n",
       "         [14., 17., 56.],\n",
       "         [14., 17., 57.],\n",
       "         [14., 17., 58.],\n",
       "         [14., 17., 59.],\n",
       "         [14., 17., 60.]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace[\"env/cumulated_reward\"].shape, workspace[\"env/cumulated_reward\"][-8:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c63b3",
   "metadata": {},
   "source": [
    "### Actions\n",
    "\n",
    "The resulting tensor of actions, with the last two actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aad76f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([61, 3]),\n",
       " tensor([[0, 1, 1],\n",
       "         [0, 1, 1]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace[\"action\"].shape, workspace[\"action\"][-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5456d39",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create a stupid agent that always outputs action 1, until the episode stops.\n",
    "Watch the content of the resulting workspace."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
