# Tabular Dynamic Programming

### Slides

[General introduction to RL](https://master-dac.isir.upmc.fr/slides_bank/intro_tab_rl.pdf)

[Markov Decision Processes](https://master-dac.isir.upmc.fr/slides_bank/mdp.pdf)

[Dynamic Programming](https://master-dac.isir.upmc.fr/slides_bank/dp.pdf)

### Videos

[Introduction (13')](https://www.youtube.com/watch?v=9gzL3QQzvQ4)

[MDPs (14')](https://www.youtube.com/watch?v=e9GxQp-LONU)

[Dynamic Programming (19')](https://www.youtube.com/watch?v=wOLBxDA6SRY)

Note about the videos: The slides are more recent. The videos note $r_t$ the reward resulting from applying action $a_t$ in state $s_t$, in the slides I switched to noting it $r_{t+1}$, which makes more sense.

### Labs

http://master-dac.isir.upmc.fr/rld/rl/01-dynamic_programming.student.ipynb

### Additional material

Convergence proofs for value iteration and policy iteration (borrowed from Sylvain Lamprier's class, in french)

https://dac.lip6.fr/wp-content/uploads/2020/10/preuves.pdf